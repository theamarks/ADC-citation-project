---
title: "Arctic Data Center Dataset Citation Analysis"
author: "Althea Marks"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: spacelab
---


### Purpose: 
Run ADC DOIs through `scythe` & compare to known DataONE metrics citations. Known ADC citations have mixed origins including DataCite, pervious `scythe` runs, and mannual additions via the ADC UI. 

### Questions: 

1) Does the addition of xDD search methods and digital library to the Scythe package improve the quality and scope of citations in the ADC?
2) Does increasing the number of sources we are searching result in more complete coverage (quality)?
- Overlap in citation among sources
- Species rarification curve inspired - start to get to a point where we can estimate the actual amount of citation out there. Dataset citations are rare enough the technique may not be applicable. 
3) Does the prevalence of data citations differ among disciplines (enviro vs SS)?
- Use ADC dicipline classifications
- Dataset citations are rare, N of classifications varies widely, need to control for sampling biases
https://zenodo.org/record/4730857#.YoaQ2WDMKrM
4) Total number of citations is extremely useful. Ground truth analysis - for a small number of datasets manually search through literature for citations. 
5) Do usage metrics (downloads and views) correlate well with citation metrics?


### Methods Overview:

1) Gather existing/known ADC dataset citations picked up by the automated DataONE metrics API
2) Get a list of all ADC dataset DOIs
3) Run all ADC dataset DOIs through `scythe` libraries
4) Compare citations from `scythe` to DataONE metrics
5) Calculate citation source overlap

### Setup

```{r setup, include=F}
knitr::opts_chunk$set(echo = T)

```

```{r directories, message=F}
output_dir <- file.path("../output")
error_dir <- file.path("../errors")
data_dir <- file.path("../data")

```

```{r package_library, message=FALSE}

library(jsonlite)
library(tidyr)
library(dplyr)
library(dataone)
library(job)
library(magrittr)
library(readr)
# library(scythe) # published version of package

# install development version of scythe package with added xdd library
devtools::install_github("dataoneorg/scythe@develop")
```

### Analysis:

#### 1. Retrieve all ADC citations with GET API request with the following request body

metrics service production endpoint: https://logproc-stage-ucsb-1.test.dataone.org/metrics

```{r get_request_citations, eval=FALSE}
{
  "metricsPage":{
    "total":0,
    "start":0,
    "count":0
  },
  "metrics":["citations"],
  "filterBy":[{
    "filterType":"repository",
    "values":["urn:node:ARCTIC"],
    "interpretAs":"list"
  },
  {
    "filterType":"month",
    "values":["01/01/2012",
              "05/24/2022"],
    "interpretAs":"range"
  }],
  "groupBy":["month"]
}
```

Example request:

```{r example_request, eval=FALSE}
https://logproc-stage-ucsb-1.test.dataone.org/metrics?metricsRequest={%22metricsPage%22:{%22total%22:0,%22start%22:0,%22count%22:0},%22metrics%22:[%22citations%22],%22filterBy%22:[{%22filterType%22:%22repository%22,%22values%22:[%22urn:node:ARCTIC%22],%22interpretAs%22:%22list%22},{%22filterType%22:%22month%22,%22values%22:[%2201/01/2012%22,%2205/24/2022%22],%22interpretAs%22:%22range%22}],%22groupBy%22:[%22month%22]
}
```

Function to retrieve dataset citations generated by DataOne metrics service (from Jeanette Clark):

```{r adc_metrics_citations_function}

metrics_citations <- function(from = as.POSIXct("2010-01-01"), to = as.POSIXct(Sys.Date())){

    from <- as.Date(from); to <- as.Date(to)
    from_q <- paste(stringr::str_pad(lubridate::month(from), 2, side = "left", pad = "0"),
                    stringr::str_pad(lubridate::day(from), 2, side = "left", pad = "0"),
                    stringr::str_pad(lubridate::year(from), 2, side = "left", pad = "0"),
                    sep = "/")

    to_q <- paste(stringr::str_pad(lubridate::month(to), 2, side = "left", pad = "0"),
                  stringr::str_pad(lubridate::day(to), 2, side = "left", pad = "0"),
                  stringr::str_pad(lubridate::year(to), 2, side = "left", pad = "0"),
                  sep = "/")

    d <- jsonlite::fromJSON(paste0('https://logproc-stage-ucsb-1.test.dataone.org/metrics?q={%22metricsPage%22:{%22total%22:0,%22start%22:0,%22count%22:0},%22metrics%22:[%22citations%22],%22filterBy%22:[{%22filterType%22:%22repository%22,%22values%22:[%22urn:node:ARCTIC%22],%22interpretAs%22:%22list%22},{%22filterType%22:%22month%22,%22values%22:[%22', from_q,'%22,%22', to_q, '%22],%22interpretAs%22:%22range%22}],%22groupBy%22:[%22month%22]}'))

    output_json <- d$resultDetails$citations # pulls citation info
    output_df <- as.data.frame(do.call(rbind, output_json), row.names = FALSE) # binds nested cit info into dataframe
   # output_clean <- rownames_to_column(output_df, var = "citation_id") # converts row names to column
    return(output_df)
}
```


```{r adc_citations, cache=TRUE}
# Run ADC API Get call, unnest target_id results to individual columns
dataone_cit <- metrics_citations()

dataone_cit <- tidyr::unnest(dataone_cit,
                          cols = c(target_id, source_id, source_url,
                                   link_publication_date, origin, title,
                                   publisher, journal, volume, page, year_of_publishing))

```

#### 2. Get all ADC DOIs from SOLR query

DataOne metrics API can only provide data package DOIs with citations, and can not provide a comprehensive list of all data package DOIs contained within the ADC. To search through all the repository metadata we query the DataONE search index (Apache SOLR search engine). SOLR is the same underlying mechanism that DataONE uses in the online tool and can create complex logical query conditions.

```{r SOLR_query, cache=TRUE}
# complete list of searchable values
#getQueryEngineDescription(cn, "solr")

# set coordinationg node
cn <- dataone::CNode("PROD")

# point to specific member node
mn <- dataone::getMNode(cn, "urn:node:ARCTIC")

# set up Solr query parameters
queryParamList <- list(q="id:doi*", 
                       fl="id,title,dateUploaded,datasource",
                       start ="0",
                       rows = "100000")
# use `q = "identifier:doi* AND (*:* NOT obsoletedBy:*)"` to only include current versions of data packages 

# send query to Solr, return results as dataframe
solr_adc_result <- dataone::query(mn, solrQuery=queryParamList, as="data.frame", parse=FALSE)

#write.csv(citations_scopus, file = paste0("../output/solr_adc_", Sys.Date(), ".csv"))
```

#### 3. Run all ADC dataset DOIs through `scythe` libraries

```{r All_ADC_DOIs}
# create vector of all ADC DOIs from solr query `result`
adc_all_dois <- c(solr_adc_result$id)
```

APIs can have request rate limits. These specific rates are often found in the API documentation or the API response headers. If request rate limits are exceeded API queries will fail.

```{r get_API_rate_limits, eval=FALSE}
# Get rate limit information from scopus

key <- scythe::scythe_get_key("scopus")
url <- paste0("https://api.elsevier.com/content/search/scopus?query=ALL:",
  "10.18739/A2M32N95V",
  paste("&APIKey=", key, sep = ""))

curlGetHeaders(url)
# shows 

# springer limits
# 300 calls/min and 5000/day

key_spring <- scythe::scythe_get_key("springer")
url_spring <- paste0("http://api.springernature.com/meta/v2/json?q=doi:10.1007/BF00627098&api_key=", key_spring)
curlGetHeaders(url_spring)
```

Run each library search in parallel in separate background jobs to keep console available to work with. By default the `job::job()` imports the global environment into the background job.

**Important Note** `scythe::scythe_set_key()` is a wrapper for the `key_ring` package. An interactive password prompt is required to access the API keys stored in `key_ring`. This *does not work* within a background job environment; your keyring needs to be temporarily unlocked with `keyring::keyring_unlock("scythe", "your password")` replace `password` with your actual keyring password. Be careful not to save, commit, or push your personal keyring password.

```{r citation_searches_background_jobs, eval=FALSE}

# Run each source/library search in a separate background job. Running a for loop will return incomplete results if API query fails, which is better than loosing all progress because of a single error in a single vector call.  

#key <- "password"

# Scopus
citations_scopus <- data.frame()


job::job({
  for (i in seq_along(adc_all_dois)) {
    # access API keys within background job environment
    keyring::keyring_unlock("scythe", key)
    # suppress errors and continue loop iteration
    result <- tryCatch(citation <- scythe::citation_search(adc_all_dois[i], "scopus"),
                       error = function(err) {
                         writeLines(paste("Errored on", "adc_all_dois[i]", "with error:", err), 
                                    file.path("errors", paste0(adc_all_dois[i], ".txt")))
                         data.frame("article_id" = NA,
                                    "article_title" = NA,
                                    "dataset_id" = adc_all_dois[i],
                                    "source" = "scopus")
                         }
                       )
    citations_scopus <- rbind(citations_scopus, result)
  }
}, title = paste0("scopus citation search ", Sys.time()))

# Springer
citations_springer <- data.frame()

job::job({
  for (i in seq_along(adc_all_dois)) {
    # access API keys within background job environment
    keyring::keyring_unlock("scythe", key)
    # suppress errors and continue loop iteration
    result <- tryCatch(citation <- scythe::citation_search(adc_all_dois[i], "springer"),
                       error = function(err) {
                         print(err)
                         data.frame("article_id" = NA,
                                    "article_title" = NA,
                                    "dataset_id" = adc_all_dois[i],
                                    "source" = "springer")
                         }
                       )
    citations_springer <- rbind(citations_springer, result)
    }
}, title = paste0("springer citation search ", Sys.time()))

# plos
citations_plos <- data.frame()

job::job({
  for (i in seq_along(adc_all_dois)) {
    # access API keys within background job environment
    keyring::keyring_unlock("scythe", key)
    # suppress errors and continue loop iteration
    result <- tryCatch(citation <- scythe::citation_search(adc_all_dois[i], "plos"),
                       error = function(err) {
                         print(err)
                         data.frame("article_id" = NA,
                                    "article_title" = NA,
                                    "dataset_id" = adc_all_dois[i],
                                    "source" = "plos")
                         }
                       )
    citations_plos <- rbind(citations_plos, result)
    }
}, title = paste0("plos citation search ", Sys.time()))


# xdd
citations_xdd <- data.frame()

job::job({
  for (i in seq_along(adc_all_dois)) {
    # access API keys within background job environment
    keyring::keyring_unlock("scythe", key)
    # suppress errors and continue loop iteration
    result <- tryCatch(citation <- scythe::citation_search(adc_all_dois[i], "xdd"),
                       error = function(err) {
                         print(err)
                         data.frame("article_id" = NA,
                                    "article_title" = NA,
                                    "dataset_id" = adc_all_dois[i],
                                    "source" = "xdd")
                         }
                       )
    citations_xdd <- rbind(citations_xdd, result)
    }
}, title = paste0("xdd citation search ", Sys.time()))
```

```{r save_scythe_results, eval=FALSE}
## Save results
write.csv(citations_scopus, file = paste0("../output/scythe_scopus_", Sys.Date(), ".csv"), row.names = F)
write.csv(citations_springer, file = paste0("../output/scythe_springer_", Sys.Date(), ".csv"), row.names = F)
write.csv(citations_plos, file = paste0("../output/scythe_plos_", Sys.Date(), ".csv"), row.names = F)
write.csv(citations_xdd, file = paste0("../output/scythe_xdd_", Sys.Date(), ".csv"), row.names = F)

```

#### 4. Compare `scythe` results to DataOne Metrics citations

```{r new_scythe_citations, message=FALSE}
# DataOne Metrics Documentation - https://app.swaggerhub.com/apis/nenuji/data-metrics/1.0.0.3 
# source_id = 'Unique identifier to the source dataset / document / article that cited the target dataset '
# target_id = 'Unique identifier to the target DATAONE dataset. This is the dataset that was cited.'
dataone_met_cit <- dataone_cit %>% 
  rename("article_id" = source_id, # rename dataone metrics citations columns to match scythe results
         "dataset_id" = target_id) %>% 
  select(article_id, dataset_id)

# Combine all scythe results from all sources (this will be cleaner eventually, like this for the sake of running)
scopus <- read_csv("../output/scythe_scopus_2022-06-30.csv")
springer <- read_csv("../output/scythe_springer_2022-06-30.csv")
plos <- read_csv("../output/scythe_plos_2022-06-30.csv")
xdd <- read_csv("../output/scythe_xdd_2022-06-30.csv")

scythe_cit <- rbind(scopus, springer, plos, xdd)

# only show citations found by scythe that do not already appear in the DataOne Metrics service
new_scythe_cit <- anti_join(scythe_cit, dataone_met_cit, by = c("article_id", "dataset_id")) %>% 
  na.omit()

sum_scythe <- new_scythe_cit %>% 
  group_by(source) %>% 
  summarise("new_citations" = n())

sum_scythe

new_cit <- unique(new_scythe_cit[,c(1,3)])
```

Pretty cool! We have `r nrow(new_cit)` unique citations (not currently in DataOne Metrics) found by my incomplete 2022-06-28 `scythe` run.

#### 5) Calculate citation source overlap/coverage

```{r citation_source_overlap, eval=F}

sources <- list(scopus, plos, xdd) # way to set this at beginning genarically if we add more sources?
citation_overlap <- data.frame()

match_source_citations <- function(source_1, source_2){
  x <- plyr::match_df(source_1, source_2, on = c("article_id", "dataset_id")) %>% 
    dplyr::mutate(matching_source = source_2$source[[1]])
  return(x)
}

for (i in 1:length(sources)){
  if(i < length(sources)){
    match <- match_source_citations(sources[i], sources[i+1])
  } 
  citation_overlap <- rbind(citation_overlap, match)
}

purrr::map_dfr()
  
test <- plyr::match_df(scopus, xdd, on = c("article_id", "dataset_id"))
```


### Next Steps:

- Analysis by discipline

[Root of ADC discipline semantics annotations](https://bioportal.bioontology.org/ontologies/ADCAD/?p=classes&conceptid=root) Classes/ID is where to look for query specifics.
"Here’s an example SOLR query that looks for two of those disciplines: `https://cn.dataone.org/cn/v2/query/solr/?q=sem_annotation:*ADCAD_00077+OR+sem_annotation:*ADCAD_00005&fl=identifier,formatId,sem_annotation`" - Matt 
Need to list every single SS ID in query list, not set up to query umbrella SS ID just yet
