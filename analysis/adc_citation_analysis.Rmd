---
title: "Arctic Data Center Dataset Citation Analysis"
author: "Althea Marks"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: spacelab
---


### Purpose: 
Run ADC DOIs through `scythe` & compare to known DataONE metrics citations. Known ADC citations have mixed origins including DataCite, pervious `scythe` runs, and mannual additions via the ADC UI. 

### Questions: 

1) Does the addition of xDD search methods and digital library to the Scythe package improve the quality and scope of citations in the ADC?
2) Does increasing the number of sources we are searching result in more complete coverage (quality)?
- Overlap in citation among sources
- Species rarification curve inspired - start to get to a point where we can estimate the actual amount of citation out there. Dataset citations are rare enough the technique may not be applicable. 
3) Does the prevalence of data citations differ among disciplines (enviro vs SS)?
- Use ADC dicipline classifications
- Dataset citations are rare, N of classifications varies widely, need to control for sampling biases
https://zenodo.org/record/4730857#.YoaQ2WDMKrM
4) Total number of citations is extremely useful. Ground truth analysis - for a small number of datasets manually search through literature for citations. 
5) Do usage metrics (downloads and views) correlate well with citation metrics?


### Methods Overview:

1) Gather existing/known ADC dataset citations picked up by the automated DataONE metrics API
2) Get a list of all ADC dataset DOIs
3) Run all ADC dataset DOIs through `scythe` libraries
4) Compare citations from `scythe` to DataONE metrics
5) Calculate citation source overlap

### Setup

```{r setup, include=F}
knitr::opts_chunk$set(echo = T)

```

```{r directories, message=F}
output_directory <- "../output"
error_directory <- "./errors"

```

```{r package_library, message=FALSE}

library(jsonlite)
library(tidyr)
library(dplyr)
library(dataone)
library(job)
library(magrittr)
library(readr)
# library(scythe) # published version of package

# install development version of scythe package with added xdd library
devtools::install_github("dataoneorg/scythe@develop")
```

### Analysis:

#### 1. Retrieve all ADC citations with GET API request with the following request body

metrics service production endpoint: https://logproc-stage-ucsb-1.test.dataone.org/metrics

```{r get_request_citations, eval=FALSE}
{
  "metricsPage":{
    "total":0,
    "start":0,
    "count":0
  },
  "metrics":["citations"],
  "filterBy":[{
    "filterType":"repository",
    "values":["urn:node:ARCTIC"],
    "interpretAs":"list"
  },
  {
    "filterType":"month",
    "values":["01/01/2012",
              "05/24/2022"],
    "interpretAs":"range"
  }],
  "groupBy":["month"]
}
```

Example request:

```{r example_request, eval=FALSE}
https://logproc-stage-ucsb-1.test.dataone.org/metrics?metricsRequest={%22metricsPage%22:{%22total%22:0,%22start%22:0,%22count%22:0},%22metrics%22:[%22citations%22],%22filterBy%22:[{%22filterType%22:%22repository%22,%22values%22:[%22urn:node:ARCTIC%22],%22interpretAs%22:%22list%22},{%22filterType%22:%22month%22,%22values%22:[%2201/01/2012%22,%2205/24/2022%22],%22interpretAs%22:%22range%22}],%22groupBy%22:[%22month%22]
}
```

Function to retrieve dataset citations generated by DataOne metrics service (from Jeanette Clark):

```{r adc_metrics_citations_function}

metrics_citations <- function(from = as.POSIXct("2010-01-01"), to = as.POSIXct(Sys.Date())){

    from <- as.Date(from); to <- as.Date(to)
    from_q <- paste(stringr::str_pad(lubridate::month(from), 2, side = "left", pad = "0"),
                    stringr::str_pad(lubridate::day(from), 2, side = "left", pad = "0"),
                    stringr::str_pad(lubridate::year(from), 2, side = "left", pad = "0"),
                    sep = "/")

    to_q <- paste(stringr::str_pad(lubridate::month(to), 2, side = "left", pad = "0"),
                  stringr::str_pad(lubridate::day(to), 2, side = "left", pad = "0"),
                  stringr::str_pad(lubridate::year(to), 2, side = "left", pad = "0"),
                  sep = "/")

    d <- jsonlite::fromJSON(paste0('https://logproc-stage-ucsb-1.test.dataone.org/metrics?q={%22metricsPage%22:{%22total%22:0,%22start%22:0,%22count%22:0},%22metrics%22:[%22citations%22],%22filterBy%22:[{%22filterType%22:%22repository%22,%22values%22:[%22urn:node:ARCTIC%22],%22interpretAs%22:%22list%22},{%22filterType%22:%22month%22,%22values%22:[%22', from_q,'%22,%22', to_q, '%22],%22interpretAs%22:%22range%22}],%22groupBy%22:[%22month%22]}'))

    output_json <- d$resultDetails$citations # pulls citation info
    output_df <- as.data.frame(do.call(rbind, output_json), row.names = FALSE) # binds nested cit info into dataframe
   # output_clean <- rownames_to_column(output_df, var = "citation_id") # converts row names to column
    return(output_df)
}
```


```{r adc_citations, cache=TRUE}
# Run ADC API Get call, unnest target_id results to individual columns
dataone_cit <- metrics_citations()

dataone_cit <- tidyr::unnest(dataone_cit,
                          cols = c(target_id, source_id, source_url,
                                   link_publication_date, origin, title,
                                   publisher, journal, volume, page, year_of_publishing))

write_csv(dataone_cit, file.path(output_directory,
                                 paste0("dataone_metrics_cit_", Sys.Date(),".csv")))

```

#### 2. Get all ADC DOIs from SOLR query

DataOne metrics API can only provide data package DOIs with citations, and can not provide a comprehensive list of all data package DOIs contained within the ADC. To search through all the repository metadata we query the DataONE search index (Apache SOLR search engine). SOLR is the same underlying mechanism that DataONE uses in the online tool and can create complex logical query conditions.

```{r SOLR_query, cache=TRUE}
# complete list of searchable values
#getQueryEngineDescription(cn, "solr")

# set coordinationg node
cn <- dataone::CNode("PROD")

# point to specific member node
mn <- dataone::getMNode(cn, "urn:node:ARCTIC")

# set up Solr query parameters
queryParamList <- list(q="id:doi*", 
                       fl="id,title,dateUploaded,datasource",
                       start ="0",
                       rows = "100000")
# use `q = "identifier:doi* AND (*:* NOT obsoletedBy:*)"` to only include current versions of data packages 

# send query to Solr, return results as dataframe
solr_adc_result <- dataone::query(mn, solrQuery=queryParamList, as="data.frame", parse=FALSE)

write.csv(solr_adc_result, file.path(output_directory, 
                                     paste0("solr_adc_", Sys.Date(), ".csv")))
```

#### 3. Run all ADC dataset DOIs through `scythe` libraries

```{r All_ADC_DOIs}
# create vector of all ADC DOIs from solr query `result`
adc_all_dois <- c(solr_adc_result$id)
```

APIs can have request rate limits. These specific rates are often found in the API documentation or the API response headers. If request rate limits are exceeded API queries will fail.

```{r get_API_rate_limits, eval=FALSE}
# Scopus request Limits
key <- scythe::scythe_get_key("scopus")
url <- paste0("https://api.elsevier.com/content/search/scopus?query=ALL:",
  "10.18739/A2M32N95V",
  paste("&APIKey=", key, sep = ""))

curlGetHeaders(url)
# [15:17] shows "X-RateLimit-Limit:", "X-RateLimit-Remaining:", and "X-RateLimit-Reset:" (Unix epoch is the number of seconds that have elapsed since January 1, 1970 at midnight UTC time minus the leap seconds)

# Springer request Limits
# 300 calls/min and 5000/day
# not found in response header, received email from springer that I was exceeding their rates above

#key_spring <- scythe::scythe_get_key("springer")
#url_spring <- paste0("http://api.springernature.com/meta/v2/json?q=doi:10.1007/BF00627098&api_key=", key_spring)
#curlGetHeaders(url_spring)
```

Run each library search in parallel in separate background jobs to keep console available to work with. By default the `job::job()` imports the global environment into the background job.

**Important Note** `scythe::scythe_set_key()` is a wrapper for the `key_ring` package. An interactive password prompt is required to access the API keys stored in `key_ring`. This *does not work* within a background job environment; your keyring needs to be temporarily unlocked with `keyring::keyring_unlock("scythe", "your password")` replace `password` with your actual keyring password. Be careful not to save, commit, or push your personal keyring password.

```{r citation_searches_background_jobs, eval=FALSE}

# Run each source/library search in a separate background job. Running a for loop will return incomplete results if API query fails, which is better than loosing all progress because of a single error in a single vector call.  

#key <- "password"

# Set up empty results data.frames
citations_scopus <- data.frame()
citations_springer <- data.frame()
citations_plos <- data.frame()
citations_xdd <- data.frame()


######### Scopus
job::job({
  for (i in seq_along(adc_all_dois)) {
    # access API keys within background job environment
    keyring::keyring_unlock("scythe", key)
    # suppress errors and continue loop iteration
    result <- tryCatch(citation <- scythe::citation_search(adc_all_dois[i], "scopus"),
                       error = function(err) {
                         writeLines(paste("Errored on", "adc_all_dois[i]", "with error:", err), 
                                    file.path(error_directory, paste0(adc_all_dois[i], ".txt")))
                         data.frame("article_id" = NA,
                                    "article_title" = NA,
                                    "dataset_id" = adc_all_dois[i],
                                    "source" = "scopus")
                         }
                       )
    citations_scopus <- rbind(citations_scopus, result)
  }
}, title = paste0("scopus citation search ", Sys.time()))

######### Springer

# divide ADC corpus into chunks less than Springer's 5,000/day request limit
springer_limit <- 4995
adc_doi_chunks <- split(adc_all_dois, ceiling(seq_along(adc_all_dois) / springer_limit))
adc_doi_chunk_vec <- vector() # empty vector to hold hold chunk names

# create x number of objects containing chunks < 4995 DOIs (based on total number of DOIs)
# And create vector of doi chunk names to reference in search for loop
for (i in 1:length(adc_doi_chunks)) {
  assign(paste0("adc_doi_chunk_", names(adc_doi_chunks)[i]), adc_doi_chunks[[i]]) 
  adc_doi_chunk_vec <- c(adc_doi_chunk_vec, paste0("adc_doi_chunk_", names(adc_doi_chunks)[i]))
}


### Need to add in pause or wait function within for loop - how long can we extend this? 3 days too long?
job::job({
  for (i in seq_along(adc_doi_chunk_1)) { 
    # access API keys within background job environment
    keyring::keyring_unlock("scythe", key)
    # suppress errors and continue loop iteration
    result <- tryCatch(citation <- scythe::citation_search(adc_doi_chunk_1[i], "springer"),
                       error = function(err) {
                         writeLines(paste("Errored on", "adc_doi_chunk_1[i]", "with error:", err), 
                                    file.path(error_directory, paste0(adc_doi_chunk_1[i], ".txt")))
                         data.frame("article_id" = NA,
                                    "article_title" = NA,
                                    "dataset_id" = adc_doi_chunk_1[i],
                                    "source" = "springer")
                         }
                       )
    citations_springer <- rbind(citations_springer, result)
    }
}, title = paste0("springer citation search 1/2", Sys.time()))

######### plos
job::job({
  for (i in seq_along(adc_all_dois)) {
    # access API keys within background job environment
    keyring::keyring_unlock("scythe", key)
    # suppress errors and continue loop iteration
    result <- tryCatch(citation <- scythe::citation_search(adc_all_dois[i], "plos"),
                       error = function(err) {
                         writeLines(paste("Errored on", "adc_all_dois[i]", "with error:", err), 
                                    file.path(error_directory, paste0(adc_all_dois[i], ".txt")))
                         data.frame("article_id" = NA,
                                    "article_title" = NA,
                                    "dataset_id" = adc_all_dois[i],
                                    "source" = "plos")
                         }
                       )
    citations_plos <- rbind(citations_plos, result)
    }
}, title = paste0("plos citation search ", Sys.time()))


########## xdd
job::job({
  for (i in seq_along(adc_all_dois)) {
    # access API keys within background job environment
    keyring::keyring_unlock("scythe", key)
    # suppress errors and continue loop iteration
    result <- tryCatch(citation <- scythe::citation_search(adc_all_dois[i], "xdd"),
                       error = function(err) {
                         writeLines(paste("Errored on", "adc_all_dois[i]", "with error:", err), 
                                    file.path(error_directory, paste0(adc_all_dois[i], ".txt")))
                         data.frame("article_id" = NA,
                                    "article_title" = NA,
                                    "dataset_id" = adc_all_dois[i],
                                    "source" = "xdd")
                         }
                       )
    citations_xdd <- rbind(citations_xdd, result)
    }
}, title = paste0("xdd citation search ", Sys.time()))
```
```{r second_springer_search}

```


```{r save_scythe_results, eval=FALSE}
## Save results as .csvs to project output folder
write.csv(citations_scopus, file.path(output_directory, paste0("scythe_scopus_", Sys.Date(), ".csv"), row.names = F)
write.csv(citations_springer, file.path(output_directory, paste0("scythe_springer_", Sys.Date(), ".csv"), row.names = F)
write.csv(citations_plos, file.path(output_directory, paste0("scythe_plos_", Sys.Date(), ".csv"), row.names = F)
write.csv(citations_xdd, file.path(output_directory, paste0("scythe_xdd_", Sys.Date(), ".csv"), row.names = F)

# Write results in .json format for easy upload to DataOne Metrics Database - need to remove duplicate citations first
scythe::write_citation_pairs(citations_scopus, file.path(output_directory, paste0("")))

```

#### 4. Compare `scythe` results to DataOne Metrics citations

```{r new_scythe_citations, message=FALSE}
# DataOne Metrics Documentation - https://app.swaggerhub.com/apis/nenuji/data-metrics/1.0.0.3 
# source_id = 'Unique identifier to the source dataset / document / article that cited the target dataset '
# target_id = 'Unique identifier to the target DATAONE dataset. This is the dataset that was cited.'
dataone_met_cit <- dataone_cit %>% 
  rename("article_id" = source_id, # rename dataone metrics citations columns to match scythe results
         "dataset_id" = target_id) %>% 
  select(article_id, dataset_id)

# Combine all scythe results from all sources (this will be cleaner eventually, like this for the sake of running)
scopus <- read_csv("../output/scythe_scopus_2022-06-30.csv")
springer <- read_csv("../output/scythe_springer_2022-06-30.csv")
plos <- read_csv("../output/scythe_plos_2022-06-30.csv")
xdd <- read_csv("../output/scythe_xdd_2022-06-30.csv")

scythe_cit <- rbind(scopus, springer, plos, xdd)

# only show citations found by scythe that do not already appear in the DataOne Metrics service
new_scythe_cit <- anti_join(scythe_cit, dataone_met_cit, by = c("article_id", "dataset_id")) %>% 
  na.omit()

sum_scythe <- new_scythe_cit %>% 
  group_by(source) %>% 
  summarise("new_citations" = n())

sum_scythe

new_cit <- unique(new_scythe_cit[,c(1,3)])
```

Pretty cool! We have `r nrow(new_cit)` unique citations (not currently in DataOne Metrics) found by my incomplete 2022-06-28 `scythe` run.

#### 5) Calculate citation source overlap/coverage

```{r citation_source_overlap, eval=F}

sources <- list(scopus, plos, xdd) # way to set this at beginning genarically if we add more sources?
citation_overlap <- data.frame()

match_source_citations <- function(source_1, source_2){
  x <- plyr::match_df(source_1, source_2, on = c("article_id", "dataset_id")) %>% 
    dplyr::mutate(matching_source = source_2$source[[1]])
  return(x)
}

for (i in 1:length(sources)){
  if(i < length(sources)){
    match <- match_source_citations(sources[i], sources[i+1])
  } 
  citation_overlap <- rbind(citation_overlap, match)
}

purrr::map_dfr()
  
test <- plyr::match_df(scopus, xdd, on = c("article_id", "dataset_id"))
```


### Next Steps:

- Analysis by discipline

[Root of ADC discipline semantics annotations](https://bioportal.bioontology.org/ontologies/ADCAD/?p=classes&conceptid=root) Classes/ID is where to look for query specifics.
"Here’s an example SOLR query that looks for two of those disciplines: `https://cn.dataone.org/cn/v2/query/solr/?q=sem_annotation:*ADCAD_00077+OR+sem_annotation:*ADCAD_00005&fl=identifier,formatId,sem_annotation`" - Matt 
Need to list every single SS ID in query list, not set up to query umbrella SS ID just yet
