---
title: |
  ![](ADC_Logo.png){width=2in}  
  
  Searching for Elusive Arctic Dataset Citations
subtitle: "Data Fellowship Project 2022"
author: 
  - name: Althea Marks
    orcid: 0000-0002-9370-9128
    email: marks@nceas.ucsb.edu
    affiliations:
      - name: University of California Santa Barbara
        department: National Center for Ecological Analysis and Synthesis
        address: 1021 Anacapa St
        city: Santa Barbara
        state: CA
        postal-code: 93101
        url: https://www.nceas.ucsb.edu/
date: "`r Sys.Date()`"
format: 
  html: 
    number-sections: true
    toc: true
    code-tools: true
    theme: cosmo 
    self-contained: true
title-block-banner: "#B5E1E7"
title-block-banner-color: "#146660"
---

## Purpose

Run ADC DOIs through `scythe` & compare to known DataONE metrics citations. Known ADC citations have mixed origins including DataCite, previous `scythe` runs, and manual additions via the ADC UI.

## Questions

1)  Does the addition of the [xDD](https://geodeepdive.org/) digital library to the [Scythe package](https://github.com/DataONEorg/scythe/tree/main) improve the quality and scope of citations in the ADC? Does increasing the number of sources we are searching result in more complete coverage (quality)?

    -   Overlap in citation among sources

    -   Species ratification curve inspired - start to get to a point where we can estimate the actual amount of citation out there. Dataset citations are rare enough the technique may not be applicable. Rarefaction.

    > The calculation of species richness for a given number of samples is based on the rarefaction curve. The rarefaction curve is a plot of the number of species against the number of samples. This curve is created by randomly re-sampling the pool of N samples several times and then plotting the average number of species found on each sample. Generally, it initially grows rapidly (as the most common species are found) and then slightly flattens (as the rarest species remain to be sampled). [source](https://www.cd-genomics.com/microbioseq/rarefaction-curve-a-measure-of-species-richness-and-diversity.html)

    *Would this be sampling the entireity of ADC DOIs?*

2)  Does the prevalence of data citations differ among disciplines (enviro vs SS)?

    -   Use ADC dicipline classifications

    -   Dataset citations are rare, N of classifications varies widely, need to control for sampling biases <https://zenodo.org/record/4730857#.YoaQ2WDMKrM>

<!-- -->

4)  Total number of citations is extremely useful. Ground truth analysis - for a small number of datasets manually search through literature for citations.

5)  Do usage metrics (downloads and views) correlate well with citation metrics?

## Methods Overview

-   Gather existing/known ADC dataset citations picked up by the automated DataONE metrics API

-   Get a list of all ADC dataset DOIs

-   Run all ADC dataset DOIs through `scythe` libraries

-   Review HTTP errors and rerun

-   Calculate citation source overlap

-   Compare citations from `scythe` to DataONE metrics

## R Setup

```{r md_setup, include=F}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE, message = FALSE)

```

```{r analysis_setup, eval=T, message=FALSE, warning=FALSE}
#| code-fold: true

# set date here. Used throughout data collection, saving, and analysis. YYYY-MM-DD
#date <- "2022-07-14"
date <- "2022-11-03"

# vector of APIs used in analysis
source_list <- c("scopus", "springer", "plos", "xdd")

# load libraries
source(file.path("./R/load_pkgs.R"))

# create directories and file paths 
source(file.path("./R/analysis_paths.R"))

```

## Search For Citations

### Current known ADC citations

Use GET API request body in DataOne Metrics Service production endpoint: <https://logproc-stage-ucsb-1.test.dataone.org/metrics> documentation: <https://app.swaggerhub.com/apis/nenuji/data-metrics/1.0.0.3>

```{r get_request_citations}
#| code-fold: true
{
  "metricsPage":{
    "total":0,
    "start":0,
    "count":0
  },
  "metrics":["citations"],
  "filterBy":[{
    "filterType":"repository",
    "values":["urn:node:ARCTIC"],
    "interpretAs":"list"
  },
  {
    "filterType":"month",
    "values":["01/01/2012",
              "05/24/2022"],
    "interpretAs":"range"
  }],
  "groupBy":["month"]
}
```

Example request:

```{r example_request}
#| code-fold: true

https://logproc-stage-ucsb-1.test.dataone.org/metrics?metricsRequest={%22metricsPage%22:{%22total%22:0,%22start%22:0,%22count%22:0},%22metrics%22:[%22citations%22],%22filterBy%22:[{%22filterType%22:%22repository%22,%22values%22:[%22urn:node:ARCTIC%22],%22interpretAs%22:%22list%22},{%22filterType%22:%22month%22,%22values%22:[%2201/01/2012%22,%2205/24/2022%22],%22interpretAs%22:%22range%22}],%22groupBy%22:[%22month%22]
}
```

Function to retrieve dataset citations generated by DataOne metrics service (from Jeanette Clark):

```{r adc_metrics_citations_function}
#| code-fold: true
metrics_citations <- function(from = as.POSIXct("2010-01-01"), to = as.POSIXct(Sys.Date())){

    from <- as.Date(from); to <- as.Date(to)
    from_q <- paste(stringr::str_pad(lubridate::month(from), 2, side = "left", pad = "0"),
                    stringr::str_pad(lubridate::day(from), 2, side = "left", pad = "0"),
                    stringr::str_pad(lubridate::year(from), 2, side = "left", pad = "0"),
                    sep = "/")

    to_q <- paste(stringr::str_pad(lubridate::month(to), 2, side = "left", pad = "0"),
                  stringr::str_pad(lubridate::day(to), 2, side = "left", pad = "0"),
                  stringr::str_pad(lubridate::year(to), 2, side = "left", pad = "0"),
                  sep = "/")

    d <- jsonlite::fromJSON(paste0('https://logproc-stage-ucsb-1.test.dataone.org/metrics?q={%22metricsPage%22:{%22total%22:0,%22start%22:0,%22count%22:0},%22metrics%22:[%22citations%22],%22filterBy%22:[{%22filterType%22:%22repository%22,%22values%22:[%22urn:node:ARCTIC%22],%22interpretAs%22:%22list%22},{%22filterType%22:%22month%22,%22values%22:[%22', from_q,'%22,%22', to_q, '%22],%22interpretAs%22:%22range%22}],%22groupBy%22:[%22month%22]}'))

    output_json <- d$resultDetails$citations # pulls citation info
    output_df <- as.data.frame(do.call(rbind, output_json), row.names = FALSE) # binds nested cit info into dataframe
   # output_clean <- rownames_to_column(output_df, var = "citation_id") # converts row names to column
    return(output_df)
}
```

```{r adc_citations, cache=TRUE}
#| code-fold: true
# Run ADC API Get call, unnest target_id results to individual columns
dataone_cit <- metrics_citations(to = as.POSIXct(date)) # use analysis date to constrain search

dataone_cit <- tidyr::unnest(dataone_cit,
                          cols = c(target_id, source_id, source_url,
                                   link_publication_date, origin, title,
                                   publisher, journal, volume, page, year_of_publishing))

write_csv(dataone_cit, file.path(output_directory,
                                 paste0("dataone_metrics_cit_", date,".csv")))
```

### Query SOLR - all ADC DOIs

DataOne metrics API can only provide data package DOIs with citations, and can not provide a comprehensive list of all data package DOIs contained within the ADC. To search through all the repository metadata we query the DataONE search index (Apache SOLR search engine). SOLR is the same underlying mechanism that DataONE uses in the online tool and can create complex logical query conditions.

::: callout-tip
Call `dataone::getQueryEngineDescription(cn, "solr")` to return a complete list of searchable SOLR values
:::

```{r SOLR_query}
#| code-fold: true

# set coordinating node
cn <- dataone::CNode("PROD")

# point to specific member node
mn <- dataone::getMNode(cn, "urn:node:ARCTIC")

# set up Solr query parameters
queryParamList <- list(q="id:doi*", 
                       fl="id,title,dateUploaded,datasource",
                       start ="0",
                       rows = "100000") # set number to definitely exceed actual number
# use `q = "identifier:doi* AND (*:* NOT obsoletedBy:*)"` to only include current versions of data packages 
# DataOne aggregates citations across dataset versions

# send query to Solr, return results as dataframe
solr_adc_result <- dataone::query(mn, solrQuery=queryParamList, as="data.frame", parse=FALSE)

write.csv(solr_adc_result, file.path(output_directory, 
                                     paste0("solr_adc_", date, ".csv")))
```

*SOLR query does not yet include date search term to align with `date` object. Using `date` to save and read in .csv*

### Run DOIs through `scythe`

```{r All_ADC_DOIs}
#| code-fold: true

# read in saved SOLR results
solr_adc_result_csv <- read_csv(file.path(output_directory, 
                                          paste0("solr_adc_", date, ".csv")))
# create vector of all ADC DOIs from solr query `result`
adc_all_dois <- c(solr_adc_result_csv$id)
```

APIs can have request rate limits. These specific rates are often found in the API documentation or the API response headers. If request rate limits are exceeded, API queries will fail.

```{r get_API_rate_limits}
#| code-fold: true
# Scopus request Limits
key_scopus <- scythe::scythe_get_key("scopus")
url <- paste0("https://api.elsevier.com/content/search/scopus?query=ALL:",
  "10.18739/A2M32N95V",
  paste("&APIKey=", key_scopus, sep = ""))

curlGetHeaders(url)
# [15:17] shows "X-RateLimit-Limit:", "X-RateLimit-Remaining:", and "X-RateLimit-Reset:" (Unix epoch is the number of seconds that have elapsed since January 1, 1970 at midnight UTC time minus the leap seconds)

# Springer request Limits
# 300 calls/min and 5000/day
# not found in response header, received email from springer that I was exceeding their rates above

#key_spring <- scythe::scythe_get_key("springer")
#url_spring <- paste0("http://api.springernature.com/meta/v2/json?q=doi:10.1007/BF00627098&api_key=", key_spring)
#curlGetHeaders(url_spring)
```

Run each library search in parallel in separate background jobs to keep console available to work with. By default `job::job()` imports the global environment into the background job.

::: callout-note
`scythe::scythe_set_key()` is a wrapper for the `key_ring` package. An interactive password prompt is required to access the API keys stored in `key_ring`. This *does not work* within a background job environment; your keyring needs to be temporarily unlocked with `keyring::keyring_unlock("scythe", "your password")` replace `password` in the next code chunk with your actual keyring password.
:::

::: callout-warning
Be careful not to save, commit, or push your personal keyring password.
:::

```{r citation_searches_background_jobs}
#| code-fold: true
# Run each source/library search in a separate background job. Running a for loop will return incomplete results if API query fails, which is better than loosing all progress because of a single error in a single vector call.  

key <- "password"

# Set up empty results data.frames
citations_scopus <- data.frame()
citations_springer <- data.frame()
citations_plos <- data.frame()
citations_xdd <- data.frame()


######### Scopus
job::job({
  for (i in seq_along(adc_all_dois)) {
    # access API keys within background job environment
    keyring::keyring_unlock("scythe", key)
    # suppress errors and continue loop iteration
    result <- tryCatch(citation <- scythe::citation_search(adc_all_dois[i], "scopus"),
                       error = function(err) {
                         data.frame("article_id" = NA,
                                    "article_title" = NA,
                                    "dataset_id" = adc_all_dois[i],
                                    "source" = paste0("scopus ", as.character(err)))
                         }
                       )
    citations_scopus <- rbind(citations_scopus, result)
    write.csv(citations_scopus, path_scopus, row.names = F)
  }
}, title = paste0("scopus citation search ", Sys.time()))


######### PLOS
job::job({
  for (i in seq_along(adc_all_dois)) {
    # access API keys within background job environment
    keyring::keyring_unlock("scythe", key)
    # suppress errors and continue loop iteration
    result <- tryCatch(citation <- scythe::citation_search(adc_all_dois[i], "plos"),
                       error = function(err) {
                         data.frame("article_id" = NA,
                                    "article_title" = NA,
                                    "dataset_id" = adc_all_dois[i],
                                    "source" = paste0("plos", as.character(err)))
                         }
                       )
    citations_plos <- rbind(citations_plos, result)
    write.csv(citations_plos, path_plos, row.names = F)
    }
}, title = paste0("plos citation search ", Sys.time()))


########## XDD
job::job({
  for (i in seq_along(adc_all_dois)) {
    # access API keys within background job environment
    keyring::keyring_unlock("scythe", key)
    # suppress errors and continue loop iteration
    result <- tryCatch(citation <- scythe::citation_search(adc_all_dois[i], "xdd"),
                       error = function(err) {
                         data.frame("article_id" = NA,
                                    "article_title" = NA,
                                    "dataset_id" = adc_all_dois[i],
                                    "source" = paste0("xdd", as.character(err)))
                         }
                       )
    citations_xdd <- rbind(citations_xdd, result)
    write.csv(citations_xdd, path_xdd, row.names = F)
    }
}, title = paste0("xdd citation search ", Sys.time()))


########## Springer
# divide ADC corpus into chunks less than Springer's 5,000/day request limit
springer_limit <- 4995
num <- seq_along(adc_all_dois)
chunk_list <- split(adc_all_dois, ceiling(num/springer_limit))


job::job({
  for(chunk in seq_along(chunk_list)){
    # pause api query for > 24hrs between chunk runs
    if(chunk != 1){Sys.sleep(87000)}
    for (i in seq_along(chunk_list[[chunk]])){ 
      # access API keys within background job environment
      keyring::keyring_unlock("scythe", key)
      # suppress errors and continue loop iteration
      result <- tryCatch(citation <- scythe::citation_search(chunk_list[[chunk]][i], "springer"),
                         error = function(err) {
                           data.frame("article_id" = NA,
                                      "article_title" = NA,
                                      "dataset_id" = chunk_list[[chunk]][i],
                                      "source" = paste0("springer ", as.character(err)))
                         }
      )
      test <- rbind(test, result)
      write.csv(citations_springer, path_springer, row.names = F)
    }
  }
}, title = paste0("springer citation search", Sys.time())
)
```

Springer's API query limits affected how we ran our search. We decided to break the list of ADC DOIs into \< 5,000 DOI chunks and run each chunk through the API with 24hrs in between the last query and starting the next DOI chunk. We could have changed the base `scythe` function `citation_search_springer()` to slow down to accommodate both request limits, but this would substantially slow down the function and make smaller DOIs queries slow and cumbersome.

```{r springer_search_throttled}
#| code-fold: true
######### Springer

# divide ADC corpus into chunks less than Springer's 5,000/day request limit
springer_limit <- 4995
length(adc_all_dois) / springer_limit
chunk_1 <- adc_all_dois[1:springer_limit]
chunk_2 <- adc_all_dois[(springer_limit+1):(springer_limit*2)]
chunk_3 <- adc_all_dois[((springer_limit*2)+1):length(adc_all_dois)]

# change "chunk_x" object to search next chunk of DOIs. Must wait 24 hrs from last request. 
doi_chunk = chunk_3

job::job({
  for (i in seq_along(doi_chunk)){ 
    # access API keys within background job environment
    keyring::keyring_unlock("scythe", key)
    # suppress errors and continue loop iteration
    result <- tryCatch(citation <- scythe::citation_search(doi_chunk[i], "springer"),
                       error = function(err) {
                         data.frame("article_id" = NA,
                                    "article_title" = NA,
                                    "dataset_id" = doi_chunk[i],
                                    "source" = paste0("springer ", as.character(err)))
                         }
                       )
    citations_springer <- rbind(citations_springer, result)
    write.csv(citations_springer, path_springer, row.names = F)
    }
}, title = paste0("springer citation search", Sys.time())
)

```

### Dealing with errors

The `tryCatch()` functions in the above search `for` loops records errors produced from any API request or scythe function. The corresponding DOIs are extracted and rerun through `scythe` a second time. When running the DOIs with errors through Scopus we discovered two bugs in the `scythe` code. The first bug was fixed [here](https://github.com/DataONEorg/scythe/commit/59bb1944bd755c2e3cd6258f02025ad1d0515723). The second bug was a query return that did not have a DOI (conference proceedings).

```{r pull_errors_rerun}
#| code-fold: true
# Extract DOIs that error during scythe queries

# read in raw results .csv into a list of dataframes
mk_result_list <- function(asource){
  path <- eval(parse(text = paste0("path_", asource)))
  if(file.exists(path)){
    assign(paste0("cits_",asource), read_csv(file.path(path)))
  }
}
results_list <- lapply(source_list, FUN = mk_result_list)
# assign source names to list elements
names(results_list) <- source_list

# pull dataframe rows that had API request errors
did_this_error <- function(adf){
  error_index <- is.na(adf$article_id)
  adf[error_index, "dataset_id"]
}
error_list <- lapply(results_list, FUN = did_this_error)

# run error DOIs back through scythe
query_errors <- function(adf, asource){
  if(nrow(adf) > 0){
    keyring::keyring_unlock("scythe", key)
    scythe::citation_search(adf$dataset_id, asource)
  } 
}  
                                 
error_query_results <- sapply(error_list, FUN = query_errors, source_list)

write_error_results <- function(adf, asource){
  if(!is.null(adf)) {
    write.csv(adf, paste0(path_error, asource, "_err_res.csv"), row.names = F)
  }}

map2(error_query_results, source_list, write_error_results)
```

::: callout-note
Running a second round of API queries using error DOIs is semi-automated above. Future script users will likely need to adjust the above code chunk to combine 1st and 2nd run results for analysis.
:::

```{r run_failed_query_dois_again}
#| code-fold: true

# This code was used during the '2022-07-08' scythe run. 
## Scopus
citations_error_scopus <- data.frame()
job::job({
  for (i in seq_along(doi_error_scopus)) {
    # access API keys within background job environment
    keyring::keyring_unlock("scythe", key)
    # suppress errors and continue loop iteration
    result <- tryCatch(citation <- scythe::citation_search(doi_error_scopus[i], "scopus"),
                       error = function(err) {
                         data.frame("article_id" = NA,
                                    "article_title" = NA,
                                    "dataset_id" = doi_error_scopus[i],
                                    "source" = paste0("scopus ", as.character(err)))
                         }
                       )
    citations_error_scopus <- rbind(citations_error_scopus, result)
  }
}, title = paste0("scopus error citation search ", Sys.time()))

# save search results from errored DOI
write.csv(citations_error_scopus,
          file.path(output_directory, paste0("scythe_", date, "_scopus_error.csv")),
          row.names = F)
# 2022-07-14 scopus errors were incorporated into cits_scopus at some point. Not reflected in this code script.

######### PLOS
citations_error_plos <- data.frame()
job::job({
  for (i in seq_along(doi_error_plos)) {
    # access API keys within background job environment
    keyring::keyring_unlock("scythe", key)
    # suppress errors and continue loop iteration
    result <- tryCatch(citation <- scythe::citation_search(doi_error_plos[i], "plos"),
                       error = function(err) {
                         data.frame("article_id" = NA,
                                    "article_title" = NA,
                                    "dataset_id" = doi_error_plos[i],
                                    "source" = paste0("plos", as.character(err)))
                         }
                       )
    citations_error_plos <- rbind(citations_error_plos, result)
    }
}, title = paste0("plos error citation search ", Sys.time()))
# empty dataframe return means no citations found and no HTTP errors
```

## Analysis / Results

### Does addition of xDD improve quality & scope of ADC dataset citations?

*Does increasing the number of sources we are searching result in more complete coverage/quality?*

```{r read_saved_scythe_results, eval=T, message=FALSE, warning=FALSE}
#| code-fold: true

# read in saved scythe results for all sources `cits_source` objects created
# reduces dependency on global environment objects - can pick up analysis here instead of rerunning scythe

for(i in source_list){
    path <- eval(parse(text = paste0("path_", i)))
    if(file.exists(path)){
      assign(paste0("cits_",i), 
             if(file.exists(paste0(path_error, i, "_err_res.csv"))){
               rbind(read_csv(file.path(path)), 
                     read_csv(file.path(paste0(path_error, i, "_err_res.csv"))))
             } else(read_csv(file.path(path)))
      )
    } else{print(paste0(i, " saved scythe results do not exsist in output directory"))
      }
    }

# read in saved combined results if already exist, create and save if not
if(file.exists(path_all)) {
  scythe_cit <- read_csv(path_all)
} else{
  scythe_cit <- rbind(cits_scopus, 
                      cits_springer,
                      cits_plos,
                      cits_xdd) %>%
    filter(!is.na(article_id)) # remove NA/error observations
           #grepl(dataset_id, pattern = "^10.18739.*")) # remove datasets not housed on the 
  write_csv(scythe_cit, path_all)
}

```

```{r add_error_results}
scythe_cit <- rbind(scythe_cit, error_query_results)
```


```{r raw_scythe_results, eval=T}
#| code-fold: true
#| label: tbl-raw
#| tbl-cap: "Raw Results from Scythe Search of ADC DOIs"

# create mini dataframe to populate total citations in summary table
scythe_total <- tibble("source" = "Total",
                       "num_cit" = length(scythe_cit$dataset_id),
                       "num_datasets" = length(unique(scythe_cit$dataset_id)))
  
# summary table + cheater total row
scythe_sum <- scythe_cit %>% 
  group_by(source) %>% 
  summarise("num_cit" = length(source),
            "num_datasets" = length(unique(dataset_id))) %>% 
  rbind(scythe_total)

scythe_sum$source <- c("PLOS", "Scopus", "Springer", "xDD", "Total")

knitr::kable(scythe_sum, 
             col.names = c("Source", "Number of Citations", "Number of Datasets"))
```

#### Do citation sources overlap in coverage?

We evaluated the redundancy in dataset citations found among sources by matching citations between source search results. **A citation is defined by the unique combination of `article_id` and `dataset_id`**. Percent overlap is the total number of citations found in a source also found in a second source, divided by the total number of citation found within the source.

```{r source_overlap, eval=TRUE}
#| code-fold: true
#| fig-cap: "Citation Source Overlap: Number of citations found in multiple sources and number of citations found uniquely in only one source."


# summarize the sources that each citation is found in for table
overlap <- scythe_cit %>% 
  group_by(dataset_id, article_id) %>% 
  summarize(source_combination = paste(source, collapse = "&")) %>% 
  group_by(source_combination) %>% 
  summarize(n = n())

# Create euler diagram of overlap
# Color blind friendly color pallet
#show_col(viridis(30, option = "C"))
overlap_color <- c("#AB2494FF", "#DE6065FF", "#FCA338FF", "#F0F921FF")
ovrlp_vec <- setNames(overlap$n, as.character(overlap$source_combination))
fit <- euler(ovrlp_vec)
plot(fit, 
     quantities = TRUE,
     fills = list(fill = overlap_color),
     labels = c("PLOS", "Scopus", "Springer", "xDD"))


```


```{r prct_overlap_table, eval=TRUE, echo=FALSE}
#| label: tbl-overlap
#| tbl-cap: "Percent overlap between Scythe sources"

# function to calculate percent of source citations also found in a second source
calc_prct_overlap <- function(source_1, source_2) {
  pairs <-
    inner_join(source_1, source_2, by = c("article_id", "dataset_id"))
  overlap <-
    (nrow(pairs) / nrow(source_1)) # percent of source_1 total also found in source_2 (pairs)
  return(overlap)
}

# build dataframe with overlap calcs
scythe_overlap_sum <- scythe_cit %>%
  mutate("citation_df" = ifelse(
    source == "plos",
    "cits_plos",
    ifelse(
      source == "scopus",
      "cits_scopus",
      ifelse(source == "springer", "cits_springer", "cits_xdd")
    )
  )) %>%
  group_by(source, citation_df) %>%
  summarise(
    "total_citations" = n(),
    "prct_in_plos" = calc_prct_overlap(eval(parse(text = citation_df)),
                                       cits_plos),
    "prct_in_scopus" = calc_prct_overlap(eval(parse(text = citation_df)),
                                         cits_scopus),
    "prct_in_springer" = calc_prct_overlap(eval(parse(text = citation_df)),
                                           cits_springer),
    "prct_in_xdd" = calc_prct_overlap(eval(parse(text = citation_df)),
                                      cits_xdd)
  ) %>%
  select(1, 3:7)

# read in saved overlap file, or write one
if(file.exists(path_overlap)){
  overlap_table <- read_csv(file.path(path_overlap))
} else{
  write_csv(scythe_overlap_sum, file.path(path_overlap))
  overlap_table <- read_csv(file.path(path_overlap))
}


# Overlap table
overlap_table$source <- c("PLOS", "Scopus", "Springer", "xDD")
overlap_table[,3:6] <- sapply(overlap_table[,3:6], function(x) x*100)

knitr::kable(overlap_table, 
             col.names = c("Source", "Total Citations", "% in PLOS", "% in Scopus", "% in Springer", "% in xDD"), 
             digits = 1)
```

Scopus found `r overlap[[which(overlap$source_combination == "scopus"), "n"]]` unique citations not found in any other digital libraries. Springer found `r overlap[[which(overlap$source_combination == "springer"), "n"]]`, PLOS `r overlap[[which(overlap$source_combination == "plos"), "n"]]`, and xDD `r overlap[[which(overlap$source_combination == "xdd"), "n"]]` unique citations respectfully. The total number of unique citations returned by `scythe` is `r sum(overlap$n)`.

### How many citations found by `scythe` are already known to DataOne Metrics Service?

`do_cit_src_07` came from Rushiraj. `do_cit_src_07` has the record of how citations entered the DataOne Metrics Service: `Crossref`, `Metrics Service Ingest`, and ORCID. `Metrics Service Ingest` is previous `scythe` runs. I cross referenced the `scythe` citation results with both DataOne metrics citation lists and look at the distribution of citation sources.

```{r new_scythe_citations, eval = T}
#| code-fold: true
#| fig-cap: "Dataset Citation Reporting Sources From the DataOne Arctic Data Center Metrics Service"
#| label: fig-dataone-metric-report

do_cit_src_07 <- readr::read_csv(file.path(data_dir, "dataone_cits_report_2022_07_25.csv"))

# source_id = 'Unique identifier to the source dataset / document / article that cited the target dataset '
# target_id = 'Unique identifier to the target DATAONE dataset. This is the dataset that was cited.'

# clean up dataone citation reporter csv. Remove extra ' from character strings
do_cit_src_07 <- as.data.frame(lapply(do_cit_src_07, gsub, pattern = "*'*", replacement = ""))

do_cit_src_07 %<>%
  rename("article_id" = source_id, # rename dataone metrics citations columns to match scythe results
         "dataset_id" = target_id) %>% 
  mutate(reporter = sub("^http.*","ORCiD", do_cit_src_07$reporter))

do_cit_source_sum <- do_cit_src_07 %>% 
  group_by(reporter) %>% 
  summarise(num_cit = n()) 

do_cit_source_fig <- do_cit_source_sum %>% 
  ggplot(aes(reporter, num_cit)) +
  geom_col() +
  coord_flip() +
  theme_minimal() +
  theme(panel.grid.major.y = element_blank(),
        axis.text.x=element_blank()) +
  scale_y_continuous(limits = c(NA, 1300)) +
  geom_text(aes(label = num_cit), hjust = -0.5) +
  labs(x = "",
       y = "Number of Citations",
       caption = "Total citations count: 2035")

do_cit_source_fig

```

```{r scythe-already-in-dataone, eval=F}
#| code-fold: true
#| label: tbl-scythe-do-overlap
#| tbl-cap: "Citations found by scythe that were previously recorded in DataOne Metrics Service"

scythe_cit_new <- anti_join(unique_citations, do_cit_src_07, by = c("article_id", "dataset_id")) %>% 
  na.omit()
# have 642 new scythe citations not found in dataone metrics

# Citations in dataone metrics that also show up in latest scythe search `unique_citations`
# These are the dataone metrics and scythe overlap citaitons 
scythe_in_dataone <- semi_join(do_cit_src_07, unique_citations, by = c("article_id", "dataset_id"))


scythe_in_do_sum <- scythe_in_dataone %>% 
  group_by(reporter) %>% 
  summarise(num_cit = n())

knitr::kable(scythe_in_do_sum, 
             col.names = c("Source", "Number of Citations"))
```

`scythe` found `x` new citations not currently in the DataOne Metrics Service. However, Table x and y indicate that citaitons found by previous scythe runs (`Metrics Service Injest`) are not being found by the current scythe run. Why are they missing? Do results from DataCite query overwrite previous `Metrics Service Ingest` observations? MIs `scythe` the only source for `Metrics Service Injest`?

*This could be a figure - proportion columns*

```{r}

```

*Query CrossRef to see if scythe results were reported*

3)  Does the prevalence of data citations differ among disciplines (enviro vs SS)?

    -   Use ADC discipline classifications
    -   Dataset citations are rare, N of classifications varies widely, need to control for sampling biases <https://zenodo.org/record/4730857#.YoaQ2WDMKrM>

4)  Total number of citations is extremely useful. Ground truth analysis - for a small number of datasets manually search through literature for citations.

5)  Do usage metrics (downloads and views) correlate well with citation metrics?

## Next Steps

    -   Analysis by discipline

[Root of ADC discipline semantics annotations](https://bioportal.bioontology.org/ontologies/ADCAD/?p=classes&conceptid=root) Classes/ID is where to look for query specifics. "Here's an example SOLR query that looks for two of those disciplines:

```
https://cn.dataone.org/cn/v2/query/solr/?q=sem_annotation:*ADCAD_00077+OR+sem_annotation:*ADCAD_00005&fl=identifier,formatId,sem_annotation
```

-   Matt Need to list every single SS ID in query list, not set up to query umbrella SS ID just yet

```{r discipline_semantics}
#| code-fold: true
# Run second Solr query to pull semantic annotations for 2022_08_10 DOIs

# set coordinationg node
cn <- dataone::CNode("PROD")

# point to specific member node
mn <- dataone::getMNode(cn, "urn:node:ARCTIC")

# set up Solr query parameters
queryParamList <- list(q = "id:doi* AND (*:* NOT obsoletedBy:*)",
                       fl = "id,title,dateUploaded,datasource,sem_annotation",
                       start ="0",
                       rows = "100000")

# send query to Solr, return results as dataframe. parse = T returns list column, F returns chr value
solr_adc_sem <- dataone::query(mn, solrQuery=queryParamList, as="data.frame", parse=T)

# POSSIBLE BREAK POINT - read in url
# read in csv with coded discipline ontology
adc_disc <- read.csv("https://raw.githubusercontent.com/NCEAS/adc-disciplines/main/adc-disciplines.csv") %>% 
  # use ontology id to build id url - add required amount of 0s to create 5 digit suffix
    mutate(an_uri = paste0("https://purl.dataone.org/odo/ADCAD_", 
                           stringr::str_pad(id, 5, "left", pad = "0")))

solr_adc_sem$category <- purrr::map(solr_adc_sem$sem_annotation, function(x){
    t <- grep("*ADCAD*", x, value = TRUE)
    cats <- c()
    for (i in 1:length(t)){
        z <- which(adc_disc$an_uri == t[i])
        cats[i] <- adc_disc$discipline[z]
        
    }
    return(cats)
})

# extract discipline categories from single column to populate new columns
adc_disc_wide <- solr_adc_sem %>% 
    unnest_wider(category, names_sep ="_") %>% 
    select(-sem_annotation)
```

```{r map_old_themes_to_ADCAD_ont}
#| code-fold: true
# key from old discipline categories to new ADCAD ontology - approved matches
themes <- read_csv(file.path("data/old_themes_ADCAD_key.csv"))

# read in csv with previous classifications
old_cat <- read.csv(file.path("data/dataset_categorization.csv"))

# replace old theme categories with new ADCAD ontology terms.
# only replacing terms that don't need review (themes df), leave the rest. 
new_cat <- old_cat %>% 
  select(url, id)

for(i in colnames(old_cat[7:11])) {
  # sequence through column names
  new_cat[[i]] <- purrr::map(old_cat[[i]], function(old_theme) {
    if (old_theme %in% themes$old) {
      # replace if old theme is listed
      new_theme <-
        themes$ADCAD[which(themes$old == old_theme)] # get corresponding new disc
      return(new_theme) # use disc ontology term to populate column
    } else{
      if (old_theme == "") {
        old_theme = NA
      }
      return(old_theme)
    } # keep old_theme if review is needed
  })
}

# unnest list columns
new_cat %<>% 
  unnest(cols = c("theme1", "theme2", "theme3", "theme4", "theme5"))

# Add re-categorization to solr query results (adc_disc_wide)
adc_disc_new <- adc_disc_wide %>% 
  left_join(new_cat, by = "id") %>% 
  mutate(url = url)

# check if there is any overlapping entries between theme1 & category_1 etc.
# overlap_cat1 <- adc_disc_new %>%
#   filter(!is.na(theme1) & !is.na(category_1))
# 
# overlap_cat2 <- adc_disc_new %>%
#   filter(!is.na(theme2) & !is.na(category_2))
# 
# overlap_cat3 <- adc_disc_new %>%
#   filter(!is.na(theme3) & !is.na(category_3))
# 
# overlap_cat4 <- adc_disc_new %>%
#   filter(!is.na(theme4) & !is.na(category_4))
# 
# overlap_cat5 <- adc_disc_new %>%
#   filter(!is.na(theme5) & !is.na(category_5))
# ^^ No overlap between two datasets

# Combine discipline category columns + DOI
adc_disc_ADCAD <- adc_disc_new %>% 
  transmute(url = url,
            id = id,
            title = title,
            disc_cat_1 = ifelse(!is.na(category_1), category_1, theme1),
            disc_cat_2 = ifelse(!is.na(category_2), category_2, theme2),
            disc_cat_3 = ifelse(!is.na(category_3), category_3, theme3),
            disc_cat_4 = ifelse(!is.na(category_4), category_4, theme4),
            disc_cat_5 = ifelse(!is.na(category_5), category_5, theme5)
  )
# Save results
# write.csv(adc_disc_ADCAD, "./output/ADC_ADCAD_disciplines.csv", row.names = F)
```
