[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Searching for Elusive Arctic Dataset Citations",
    "section": "",
    "text": "Run ADC DOIs through scythe & compare to known DataONE metrics citations. Known ADC citations have mixed origins including DataCite, pervious scythe runs, and mannual additions via the ADC UI."
  },
  {
    "objectID": "index.html#questions",
    "href": "index.html#questions",
    "title": "Searching for Elusive Arctic Dataset Citations",
    "section": "2 Questions",
    "text": "2 Questions\n\nDoes the addition of the xDD digital library to the Scythe package improve the quality and scope of citations in the ADC? Does increasing the number of sources we are searching result in more complete coverage (quality)?\n\nOverlap in citation among sources\nSpecies ratification curve inspired - start to get to a point where we can estimate the actual amount of citation out there. Dataset citations are rare enough the technique may not be applicable. Rarefaction.\n\n\nThe calculation of species richness for a given number of samples is based on the rarefaction curve. The rarefaction curve is a plot of the number of species against the number of samples. This curve is created by randomly re-sampling the pool of N samples several times and then plotting the average number of species found on each sample. Generally, it initially grows rapidly (as the most common species are found) and then slightly flattens (as the rarest species remain to be sampled). source\n\nWould this be sampling the entireity of ADC DOIs?\nDoes the prevalence of data citations differ among disciplines (enviro vs SS)?\n\nUse ADC dicipline classifications\nDataset citations are rare, N of classifications varies widely, need to control for sampling biases https://zenodo.org/record/4730857#.YoaQ2WDMKrM\n\n\n\n\nTotal number of citations is extremely useful. Ground truth analysis - for a small number of datasets manually search through literature for citations.\nDo usage metrics (downloads and views) correlate well with citation metrics?"
  },
  {
    "objectID": "index.html#methods-overview",
    "href": "index.html#methods-overview",
    "title": "Searching for Elusive Arctic Dataset Citations",
    "section": "3 Methods Overview",
    "text": "3 Methods Overview\n\nGather existing/known ADC dataset citations picked up by the automated DataONE metrics API\nGet a list of all ADC dataset DOIs\nRun all ADC dataset DOIs through scythe libraries\nReview HTTP errors and rerun\nCalculate citation source overlap\nCompare citations from scythe to DataONE metrics"
  },
  {
    "objectID": "index.html#setup",
    "href": "index.html#setup",
    "title": "Searching for Elusive Arctic Dataset Citations",
    "section": "4 Setup",
    "text": "4 Setup\n\n\nCode\ndir.create(path = \"./output\", showWarnings = F)\noutput_directory <- file.path(\"./output\")\n\n\n\n\nCode\nlibrary(readr)\nlibrary(jsonlite)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(dataone)\nlibrary(job)\nlibrary(magrittr)\nlibrary(readr)\nlibrary(stringr)\nlibrary(purrr)\nlibrary(ggplot2)\n# library(scythe) # published version of package\n\n# install development version of scythe package with added xdd library\ndevtools::install_github(\"dataoneorg/scythe@develop\")\n\n\n\nmy_theme <-"
  },
  {
    "objectID": "index.html#data-collection",
    "href": "index.html#data-collection",
    "title": "Searching for Elusive Arctic Dataset Citations",
    "section": "5 Data Collection",
    "text": "5 Data Collection\n\n5.1 Retrieve all ADC citations with GET API request body\nmetrics service production endpoint: https://logproc-stage-ucsb-1.test.dataone.org/metrics documentation: https://app.swaggerhub.com/apis/nenuji/data-metrics/1.0.0.3\n\n\nCode\n{\n  \"metricsPage\":{\n    \"total\":0,\n    \"start\":0,\n    \"count\":0\n  },\n  \"metrics\":[\"citations\"],\n  \"filterBy\":[{\n    \"filterType\":\"repository\",\n    \"values\":[\"urn:node:ARCTIC\"],\n    \"interpretAs\":\"list\"\n  },\n  {\n    \"filterType\":\"month\",\n    \"values\":[\"01/01/2012\",\n              \"05/24/2022\"],\n    \"interpretAs\":\"range\"\n  }],\n  \"groupBy\":[\"month\"]\n}\n\n\nExample request:\n\n\nCode\nhttps://logproc-stage-ucsb-1.test.dataone.org/metrics?metricsRequest={%22metricsPage%22:{%22total%22:0,%22start%22:0,%22count%22:0},%22metrics%22:[%22citations%22],%22filterBy%22:[{%22filterType%22:%22repository%22,%22values%22:[%22urn:node:ARCTIC%22],%22interpretAs%22:%22list%22},{%22filterType%22:%22month%22,%22values%22:[%2201/01/2012%22,%2205/24/2022%22],%22interpretAs%22:%22range%22}],%22groupBy%22:[%22month%22]\n}\n\n\nFunction to retrieve dataset citations generated by DataOne metrics service (from Jeanette Clark):\n\n\nCode\nmetrics_citations <- function(from = as.POSIXct(\"2010-01-01\"), to = as.POSIXct(Sys.Date())){\n\n    from <- as.Date(from); to <- as.Date(to)\n    from_q <- paste(stringr::str_pad(lubridate::month(from), 2, side = \"left\", pad = \"0\"),\n                    stringr::str_pad(lubridate::day(from), 2, side = \"left\", pad = \"0\"),\n                    stringr::str_pad(lubridate::year(from), 2, side = \"left\", pad = \"0\"),\n                    sep = \"/\")\n\n    to_q <- paste(stringr::str_pad(lubridate::month(to), 2, side = \"left\", pad = \"0\"),\n                  stringr::str_pad(lubridate::day(to), 2, side = \"left\", pad = \"0\"),\n                  stringr::str_pad(lubridate::year(to), 2, side = \"left\", pad = \"0\"),\n                  sep = \"/\")\n\n    d <- jsonlite::fromJSON(paste0('https://logproc-stage-ucsb-1.test.dataone.org/metrics?q={%22metricsPage%22:{%22total%22:0,%22start%22:0,%22count%22:0},%22metrics%22:[%22citations%22],%22filterBy%22:[{%22filterType%22:%22repository%22,%22values%22:[%22urn:node:ARCTIC%22],%22interpretAs%22:%22list%22},{%22filterType%22:%22month%22,%22values%22:[%22', from_q,'%22,%22', to_q, '%22],%22interpretAs%22:%22range%22}],%22groupBy%22:[%22month%22]}'))\n\n    output_json <- d$resultDetails$citations # pulls citation info\n    output_df <- as.data.frame(do.call(rbind, output_json), row.names = FALSE) # binds nested cit info into dataframe\n   # output_clean <- rownames_to_column(output_df, var = \"citation_id\") # converts row names to column\n    return(output_df)\n}\n\n\n\n\nCode\n# Run ADC API Get call, unnest target_id results to individual columns\n\ndataone_cit <- metrics_citations()\n\ndataone_cit <- tidyr::unnest(dataone_cit,\n                          cols = c(target_id, source_id, source_url,\n                                   link_publication_date, origin, title,\n                                   publisher, journal, volume, page, year_of_publishing))\n\nwrite_csv(dataone_cit, file.path(output_directory,\n                                 paste0(\"dataone_metrics_cit_\", Sys.Date(),\".csv\")))\n\n\n\n\n5.2 Get all ADC DOIs from SOLR query\nDataOne metrics API can only provide data package DOIs with citations, and can not provide a comprehensive list of all data package DOIs contained within the ADC. To search through all the repository metadata we query the DataONE search index (Apache SOLR search engine). SOLR is the same underlying mechanism that DataONE uses in the online tool and can create complex logical query conditions.\n\n\nCode\n# complete list of searchable values\n#getQueryEngineDescription(cn, \"solr\")\n\n# set coordinationg node\ncn <- dataone::CNode(\"PROD\")\n\n# point to specific member node\nmn <- dataone::getMNode(cn, \"urn:node:ARCTIC\")\n\n# set up Solr query parameters\nqueryParamList <- list(q=\"id:doi*\", \n                       fl=\"id,title,dateUploaded,datasource\",\n                       start =\"0\",\n                       rows = \"100000\")\n# use `q = \"identifier:doi* AND (*:* NOT obsoletedBy:*)\"` to only include current versions of data packages \n\n# send query to Solr, return results as dataframe\nsolr_adc_result <- dataone::query(mn, solrQuery=queryParamList, as=\"data.frame\", parse=FALSE)\n\nwrite.csv(solr_adc_result, file.path(output_directory, \n                                     paste0(\"solr_adc_\", Sys.Date(), \".csv\")))\n\n\n\n\n5.3 Run all ADC dataset DOIs through scythe libraries\n\n\nCode\n# create vector of all ADC DOIs from solr query `result`\nadc_all_dois <- c(solr_adc_result$id)\n\n\nAPIs can have request rate limits. These specific rates are often found in the API documentation or the API response headers. If request rate limits are exceeded, API queries will fail.\n\n\nCode\n# Scopus request Limits\nkey_scopus <- scythe::scythe_get_key(\"scopus\")\nurl <- paste0(\"https://api.elsevier.com/content/search/scopus?query=ALL:\",\n  \"10.18739/A2M32N95V\",\n  paste(\"&APIKey=\", key_scopus, sep = \"\"))\n\ncurlGetHeaders(url)\n# [15:17] shows \"X-RateLimit-Limit:\", \"X-RateLimit-Remaining:\", and \"X-RateLimit-Reset:\" (Unix epoch is the number of seconds that have elapsed since January 1, 1970 at midnight UTC time minus the leap seconds)\n\n# Springer request Limits\n# 300 calls/min and 5000/day\n# not found in response header, received email from springer that I was exceeding their rates above\n\n#key_spring <- scythe::scythe_get_key(\"springer\")\n#url_spring <- paste0(\"http://api.springernature.com/meta/v2/json?q=doi:10.1007/BF00627098&api_key=\", key_spring)\n#curlGetHeaders(url_spring)\n\n\nRun each library search in parallel in separate background jobs to keep console available to work with. By default the job::job() imports the global environment into the background job.\n\n\n\n\n\n\nNote\n\n\n\nscythe::scythe_set_key() is a wrapper for the key_ring package. An interactive password prompt is required to access the API keys stored in key_ring. This does not work within a background job environment; your keyring needs to be temporarily unlocked with keyring::keyring_unlock(\"scythe\", \"your password\") replace password with your actual keyring password.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBe careful not to save, commit, or push your personal keyring password.\n\n\n\n\nCode\n# Run each source/library search in a separate background job. Running a for loop will return incomplete results if API query fails, which is better than loosing all progress because of a single error in a single vector call.  \n\nkey <- \"password\"\n\n# Set up empty results data.frames\ncitations_scopus <- data.frame()\ncitations_springer <- data.frame()\ncitations_plos <- data.frame()\ncitations_xdd <- data.frame()\n\n\n######### Scopus\njob::job({\n  for (i in seq_along(adc_all_dois)) {\n    # access API keys within background job environment\n    keyring::keyring_unlock(\"scythe\", key)\n    # suppress errors and continue loop iteration\n    result <- tryCatch(citation <- scythe::citation_search(adc_all_dois[i], \"scopus\"),\n                       error = function(err) {\n                         data.frame(\"article_id\" = NA,\n                                    \"article_title\" = NA,\n                                    \"dataset_id\" = adc_all_dois[i],\n                                    \"source\" = paste0(\"scopus \", as.character(err)))\n                         }\n                       )\n    citations_scopus <- rbind(citations_scopus, result)\n  }\n}, title = paste0(\"scopus citation search \", Sys.time()))\n\n\n######### PLOS\njob::job({\n  for (i in seq_along(adc_all_dois)) {\n    # access API keys within background job environment\n    keyring::keyring_unlock(\"scythe\", key)\n    # suppress errors and continue loop iteration\n    result <- tryCatch(citation <- scythe::citation_search(adc_all_dois[i], \"plos\"),\n                       error = function(err) {\n                         data.frame(\"article_id\" = NA,\n                                    \"article_title\" = NA,\n                                    \"dataset_id\" = adc_all_dois[i],\n                                    \"source\" = paste0(\"plos\", as.character(err)))\n                         }\n                       )\n    citations_plos <- rbind(citations_plos, result)\n    }\n}, title = paste0(\"plos citation search \", Sys.time()))\n\n\n########## XDD\njob::job({\n  for (i in seq_along(adc_all_dois)) {\n    # access API keys within background job environment\n    keyring::keyring_unlock(\"scythe\", key)\n    # suppress errors and continue loop iteration\n    result <- tryCatch(citation <- scythe::citation_search(adc_all_dois[i], \"xdd\"),\n                       error = function(err) {\n                         data.frame(\"article_id\" = NA,\n                                    \"article_title\" = NA,\n                                    \"dataset_id\" = adc_all_dois[i],\n                                    \"source\" = paste0(\"xdd\", as.character(err)))\n                         }\n                       )\n    citations_xdd <- rbind(citations_xdd, result)\n    }\n}, title = paste0(\"xdd citation search \", Sys.time()))\n\n\nSpringer’s API query limits affected how we ran our search. We decided to break the list of ADC DOIs into < 5,000 DOI chunks and run each chunk manually through the API with 24hrs in between the last query and starting the next DOI chunk. We could have changed the base scythe function citation_search_springer() to slow down to accommodate both request limits, but this would substantially slow down the function and make smaller DOIs queries slow and cumbersome.\n\n\nCode\n######### Springer\n\n# divide ADC corpus into chunks less than Springer's 5,000/day request limit\nspringer_limit <- 4995\nlength(adc_all_dois) / springer_limit\nchunk_1 <- adc_all_dois[1:springer_limit]\nchunk_2 <- adc_all_dois[springer_limit:(springer_limit*2)]\nchunk_3 <- adc_all_dois[(springer_limit*2):length(adc_all_dois)]\n\n# change \"chunk_x\" object to search next chunk of DOIs. Must wait 24 hrs from last request. \ndoi_chunk = chunk_1\n\njob::job({\n  for (i in seq_along(doi_chunk)){ \n    # access API keys within background job environment\n    keyring::keyring_unlock(\"scythe\", key)\n    # suppress errors and continue loop iteration\n    result <- tryCatch(citation <- scythe::citation_search(doi_chunk[i], \"springer\"),\n                       error = function(err) {\n                         data.frame(\"article_id\" = NA,\n                                    \"article_title\" = NA,\n                                    \"dataset_id\" = doi_chunk[i],\n                                    \"source\" = paste0(\"springer \", as.character(err)))\n                         }\n                       )\n    citations_springer <- rbind(citations_springer, result)\n    }\n}, title = paste0(\"springer citation search\", Sys.time())\n)\n\n\n\n\n5.4 Dealing with API request errors\nThe tryCatch() functions in the above search for loops records errors produced from any API request. The corresponding DOIs can be aggregated and rerun through scythe. The code below does this. When running the DOIs with HTTP errors through Scopus a second time we discovered a small bug in the scythe code which was subsequently fixed.\n\n\nCode\ndoi_error_scopus <- citations_scopus[is.na(citations_scopus$article_id),\"dataset_id\"]$dataset_id\ndoi_error_springer <- citations_springer[is.na(citations_springer$article_id), \"dataset_id\"]$dataset_id # no errors\ndoi_error_plos <- citations_plos[is.na(citations_plos$article_id), \"dataset_id\"]$dataset_id\ndoi_error_xdd <- citations_xdd[is.na(citations_xdd$article_id), \"dataset_id\"]$dataset_id # no errors\n\n######### Scopus # this isn't working\ncitations_error_scopus <- data.frame()\njob::job({\n  for (i in seq_along(doi_error_scopus)) {\n    # access API keys within background job environment\n    keyring::keyring_unlock(\"scythe\", key)\n    # suppress errors and continue loop iteration\n    result <- tryCatch(citation <- scythe::citation_search(doi_error_scopus[i], \"scopus\"),\n                       error = function(err) {\n                         data.frame(\"article_id\" = NA,\n                                    \"article_title\" = NA,\n                                    \"dataset_id\" = doi_error_scopus[i],\n                                    \"source\" = paste0(\"scopus \", as.character(err)))\n                         }\n                       )\n    citations_error_scopus <- rbind(citations_error_scopus, result)\n  }\n}, title = paste0(\"scopus error citation search \", Sys.time()))\n\n# save search results from errored DOI\nwrite_csv(citations_error_scopus, \"./output/scythe_2022-07-14_scopus_error.csv\")\n# add error citations results to main scopus results data.frame\ncitations_scopus <- rbind(citations_scopus, citations_error_scopus)\n\n######### PLOS\ncitations_error_plos <- data.frame()\njob::job({\n  for (i in seq_along(doi_error_plos)) {\n    # access API keys within background job environment\n    keyring::keyring_unlock(\"scythe\", key)\n    # suppress errors and continue loop iteration\n    result <- tryCatch(citation <- scythe::citation_search(doi_error_plos[i], \"plos\"),\n                       error = function(err) {\n                         data.frame(\"article_id\" = NA,\n                                    \"article_title\" = NA,\n                                    \"dataset_id\" = doi_error_plos[i],\n                                    \"source\" = paste0(\"plos\", as.character(err)))\n                         }\n                       )\n    citations_error_plos <- rbind(citations_error_plos, result)\n    }\n}, title = paste0(\"plos error citation search \", Sys.time()))\n# empty dataframe return means no citations found and no HTTP errors\n\n\n\n\nCode\n## Save results as .csv's to project output folder\ndate <- \"2022-07-14\"\n\nwrite.csv(citations_scopus,\n          file.path(output_directory, paste0(\"scythe_\", date, \"_scopus_all.csv\")),\n          row.names = F)\nwrite.csv(citations_springer,\n          file.path(output_directory, paste0(\"scythe_\", date, \"_springer_all.csv\")),\n          row.names = F)\nwrite.csv(citations_plos,\n          file.path(output_directory, paste0(\"scythe_\", date, \"_plos_all.csv\")),\n          row.names = F)\nwrite.csv(citations_xdd,\n          file.path(output_directory, paste0(\"scythe_\", date, \"_xdd_all.csv\")),\n          row.names = F)"
  },
  {
    "objectID": "index.html#analysis-results",
    "href": "index.html#analysis-results",
    "title": "Searching for Elusive Arctic Dataset Citations",
    "section": "6 Analysis / Results",
    "text": "6 Analysis / Results\n\n6.1 1. Calculate citation source overlap\nDoes the addition of the xDD digital library to the Scythe package improve the quality and scope of citations in the ADC? Does increaseing the number of sources we are searching result in more complete coverage (quality)?\nWe evaluated the redundancy in citations found among sources by matching citations between source search results. A citation is defined by the unique combination of article_id and dataset_id. Percent overlap is the percent of total citations found in a source also found in a second source.\n\n\nCode\nscythe_cit <-\n  rbind(citations_scopus,\n        citations_springer,\n        citations_plos,\n        citations_xdd) %>%\n  filter(!is.na(article_id)) # remove NA/error observations\n\nwrite_csv(scythe_cit, file.path(output_directory, paste0(\"scythe_\", date, \"_citations_all.csv\")))\n\nscythe_sum <- scythe_cit %>%\n  group_by(source) %>%\n  summarise(\"total_citations_found\" = n())\n\n# match article_id and dataset_id columns among results\noverlap_plos_scopus <-\n  inner_join(citations_plos,\n             citations_scopus,\n             by = c(\"article_id\", \"dataset_id\"))\noverlap_plos_springer <-\n  inner_join(citations_plos,\n             citations_springer,\n             by = c(\"article_id\", \"dataset_id\"))\noverlap_plos_xdd <-\n  inner_join(citations_plos,\n             citations_xdd,\n             by = c(\"article_id\", \"dataset_id\"))\noverlap_scopus_springer <-\n  inner_join(citations_scopus,\n             citations_springer,\n             by = c(\"article_id\", \"dataset_id\"))\noverlap_scopus_xdd <-\n  inner_join(citations_scopus,\n             citations_xdd,\n             by = c(\"article_id\", \"dataset_id\"))\noverlap_springer_xdd <-\n  inner_join(citations_springer,\n             citations_xdd,\n             by = c(\"article_id\", \"dataset_id\"))\n\n# function to calculate percent of source citations also found in a second source\ncalc_prct_overlap <- function(source_1, source_2) {\n  pairs <-\n    inner_join(source_1, source_2, by = c(\"article_id\", \"dataset_id\"))\n  overlap <-\n    nrow(pairs) / nrow(source_1) # percent of source_1 total also found in source_2 (pairs)\n  return(overlap)\n}\n\nscythe_overlap_sum <- scythe_cit %>%\n  mutate(\"citation_df\" = ifelse(\n    source == \"plos\",\n    \"citations_plos\",\n    ifelse(\n      source == \"scopus\",\n      \"citations_scopus\",\n      ifelse(source == \"springer\", \"citations_springer\", \"citations_xdd\")\n    )\n  )) %>%\n  group_by(source, citation_df) %>%\n  summarise(\n    \"total_citations\" = n(),\n    \"prct_in_plos\" = calc_prct_overlap(eval(parse(text = citation_df)), \n                                              citations_plos),\n    \"prct_in_scopus\" = calc_prct_overlap(eval(parse(text = citation_df)), \n                                                citations_scopus),\n    \"prct_in_springer\" = calc_prct_overlap(eval(parse(text = citation_df)), \n                                                  citations_springer),\n    \"prct_in_xdd\" = calc_prct_overlap(eval(parse(text = citation_df)), \n                                             citations_xdd)\n  ) %>%\n  select(1, 3:7)\n\n# Save summary table to output folder\nwrite.csv(scythe_overlap_sum,\n          file.path(\n            output_directory,\n            paste0(\"scythe_\", date, \"_prct_overlap.csv\")\n          ),\n          row.names = F)\n\n\n\n6.1.1 How many completely unique citations found in each source?\n\n\nCode\ncitations_plos <- readr::read_csv(file.path(output_directory, \"scythe_2022-07-14_plos.csv\"))\ncitations_scopus <- readr::read_csv(file.path(output_directory, \"scythe_2022-07-14_scopus.csv\"))\ncitations_springer <- readr::read_csv(file.path(output_directory, \"scythe_2022-07-14_springer.csv\"))\ncitations_xdd <- readr::read_csv(file.path(output_directory, \"scythe_2022-07-14_xdd.csv\"))\n\nunique_plos <- citations_plos %>% \n  anti_join(citations_scopus, by = c(\"article_id\", \"dataset_id\")) %>% \n  anti_join(citations_springer, by = c(\"article_id\", \"dataset_id\")) %>% \n  anti_join(citations_xdd, by = c(\"article_id\", \"dataset_id\")) %>% \n  filter(!is.na(article_id))\n\nunique_scopus <- citations_scopus %>% \n  anti_join(citations_plos, by = c(\"article_id\", \"dataset_id\")) %>% \n  anti_join(citations_springer, by = c(\"article_id\", \"dataset_id\")) %>% \n  anti_join(citations_xdd, by = c(\"article_id\", \"dataset_id\")) %>% \n  filter(!is.na(article_id))\n\nunique_springer <- citations_springer %>% \n  anti_join(citations_plos, by = c(\"article_id\", \"dataset_id\")) %>% \n  anti_join(citations_scopus, by = c(\"article_id\", \"dataset_id\")) %>% \n  anti_join(citations_xdd, by = c(\"article_id\", \"dataset_id\")) %>% \n  filter(!is.na(article_id))\n  \nunique_xdd <- citations_xdd %>% \n  anti_join(citations_plos, by = c(\"article_id\", \"dataset_id\")) %>% \n  anti_join(citations_scopus, by = c(\"article_id\", \"dataset_id\")) %>% \n  anti_join(citations_springer, by = c(\"article_id\", \"dataset_id\")) %>% \n  filter(!is.na(article_id))\n\n# combine unique citation tables\nunique_citations <- rbind(unique_plos, unique_scopus, unique_springer, unique_xdd)\n\n\n\n\n\n6.2 Compare scythe results to DataOne Metrics citations\ndataone_cit_source came from Rushiraj with 2035 observations. I am still confused on the back end set up of how these citations are truely being generated. dataone_cit_source has the record of how citations entered the DataOne Metrics Service: Crossref, Metrics Service Ingest, and ORCID. Metrics Service Ingest is previous scythe runs. I can cross compare the scythe citation results with both DataONe metrics citation lists and look at the distribution of citation sources.\n\n\nCode\ndataone_cit_source <- readr::read_csv(file.path(\"data/dataone_citations_reporter.csv\"))\n\n# source_id = 'Unique identifier to the source dataset / document / article that cited the target dataset '\n# target_id = 'Unique identifier to the target DATAONE dataset. This is the dataset that was cited.'\n\n# clean up dataone citation reporter csv. Remove extra ' from character strings\ndataone_cit_source <- as.data.frame(lapply(dataone_cit_source, gsub, pattern = \"*'*\", replacement = \"\"))\n\ndataone_cit_source %<>%\n  rename(\"article_id\" = source_id, # rename dataone metrics citations columns to match scythe results\n         \"dataset_id\" = target_id) %>% \n  mutate(reporter = sub(\"^http.*\",\"ORCiD\", dataone_cit_source$reporter))\n\ndo_cit_source_sum <- dataone_cit_source %>% \n  group_by(reporter) %>% \n  summarise(num_cit = n()) \n\ndo_cit_source_fig <- do_cit_source_sum %>% \n  ggplot(aes(reporter, num_cit)) +\n  geom_col() +\n  coord_flip() +\n  theme_minimal() +\n  theme(panel.grid.major.y = element_blank()) +\n  labs(y = \"Citation Reporter Source\",\n       x = \"Number of Citations\",\n       title = \"Dataset Citation Reporting Sources\",\n       subtitle = \"From the DataOne Arctic Data Center Metrics Service\",\n       caption = \"Total citations count: 2035\")\n\ndo_cit_source_fig\n\n\n\n\n\nFigure 1: Dataset Citations in DataOne Metrics Service - Citation Reporting Source\n\n\n\n\n\n\nCode\nscythe_cit_new <- anti_join(unique_citations, dataone_cit_source, by = c(\"article_id\", \"dataset_id\")) %>% \n  na.omit()\n# have 642 new scythe citations not found in dataone metrics\n\n# Citations in dataone metrics that also show up in latest scythe search `unique_citations`\n# These are the dataone metrics and scythe overlap citaitons \nscythe_in_dataone <- semi_join(dataone_cit_source, unique_citations, by = c(\"article_id\", \"dataset_id\"))\n\n\nscythe_in_do_sum <- scythe_in_dataone %>% \n  group_by(reporter) %>% \n  summarise(num_cit = n())\n\nknitr::kable(scythe_in_do_sum, \n             col.names = c(\"Source\", \"Number of Citations\"))\n\n\n\n\nTable 1: Citations found by scythe that were previously recorded in DataOne Metrics Service\n\n\nSource\nNumber of Citations\n\n\n\n\nCrossref\n5\n\n\nMetrics Service Ingest\n126\n\n\nORCiD\n18\n\n\n\n\n\n\nscythe found 642 new citations not currently in the DataOne Metrics Service. However, Table x and y indicate that citaitons found by previous scythe runs (Metrics Service Injest) are not being found by the current scythe run. Why are they missing? Do results from DataCite query overwrite previous Metrics Service Ingest observations? MIs scythe the only source for Metrics Service Injest?\nThis could be a figure - proportion columns\n\n\n\nQuery CrossRef to see if scythe results were reported\n\nDoes the addition of the xDD digital library to the Scythe package improve the quality and scope of citations in the ADC?\n\nHow many unique citations were found in xDD and not in any of the other sources?\n\n\n\n\n\n\nTable 2: Number of citations only found within each source\n\n\nSource\nUnique Citations\n\n\n\n\nPLOS\n27\n\n\nScopus\n453\n\n\nSpringer\n105\n\n\nxDD\n202\n\n\nTotal\n787\n\n\n\n\n\n\nxDD found 202 unique citations not found in Scoupus, Springer, or PLOS digital libraries. The total number of unique citations returned by scythe is 787. ## new to DataOne Metric service (should be CrossRef & ORCID reports in dataone_citations_reporter.csv)\n\nDoes increasing the number of sources we are searching result in more complete coverage (quality)?\n\n\n\n\n\nTable 3: Percent overlap between Scythe sources\n\n\n\n\n\n\n\n\n\n\nSource\nTotal Citations\n% in PLOS\n% in Scopus\n% in Springer\n% in xDD\n\n\n\n\nPLOS\n38\n1.000\n0.177\n0.000\n0.000\n\n\nScopus\n644\n0.017\n1.000\n0.092\n0.185\n\n\nSpringer\n166\n0.000\n0.361\n1.000\n0.006\n\n\nxDD\n323\n0.000\n0.372\n0.003\n1.000\n\n\n\n\n\n\n\nDoes the prevalence of data citations differ among disciplines (enviro vs SS)?\n\nUse ADC discipline classifications\nDataset citations are rare, N of classifications varies widely, need to control for sampling biases https://zenodo.org/record/4730857#.YoaQ2WDMKrM\n\nTotal number of citations is extremely useful. Ground truth analysis - for a small number of datasets manually search through literature for citations.\nDo usage metrics (downloads and views) correlate well with citation metrics?"
  },
  {
    "objectID": "index.html#next-steps",
    "href": "index.html#next-steps",
    "title": "Searching for Elusive Arctic Dataset Citations",
    "section": "7 Next Steps",
    "text": "7 Next Steps\n-   Analysis by discipline\nRoot of ADC discipline semantics annotations Classes/ID is where to look for query specifics. “Here’s an example SOLR query that looks for two of those disciplines:\n\nhttps://cn.dataone.org/cn/v2/query/solr/?q=sem_annotation:*ADCAD_00077+OR+sem_annotation:*ADCAD_00005&fl=identifier,formatId,sem_annotation\n\n\nMatt Need to list every single SS ID in query list, not set up to query umbrella SS ID just yet\n\n\n\nCode\n# Run second Solr query to pull semantic annotations for 2022_08_10 DOIs\n\n# set coordinationg node\ncn <- dataone::CNode(\"PROD\")\n\n# point to specific member node\nmn <- dataone::getMNode(cn, \"urn:node:ARCTIC\")\n\n# set up Solr query parameters\nqueryParamList <- list(q = \"id:doi* AND (*:* NOT obsoletedBy:*)\",\n                       fl = \"id,title,dateUploaded,datasource,sem_annotation\",\n                       start =\"0\",\n                       rows = \"100000\")\n\n# send query to Solr, return results as dataframe. parse = T returns list column, F returns chr value\nsolr_adc_sem <- dataone::query(mn, solrQuery=queryParamList, as=\"data.frame\", parse=T)\n\n# read in csv with coded discipline ontology\nadc_disc <- read.csv(\"https://raw.githubusercontent.com/NCEAS/adc-disciplines/main/adc-disciplines.csv\") %>% \n  # use ontology id to build id url - add required amount of 0s to create 5 digit suffix\n    mutate(an_uri = paste0(\"https://purl.dataone.org/odo/ADCAD_\", \n                           stringr::str_pad(id, 5, \"left\", pad = \"0\")))\n\nsolr_adc_sem$category <- purrr::map(solr_adc_sem$sem_annotation, function(x){\n    t <- grep(\"*ADCAD*\", x, value = TRUE)\n    cats <- c()\n    for (i in 1:length(t)){\n        z <- which(adc_disc$an_uri == t[i])\n        cats[i] <- adc_disc$discipline[z]\n        \n    }\n    return(cats)\n})\n\n# extract discipline categories from single column to populate new columns\nadc_disc_wide <- solr_adc_sem %>% \n    unnest_wider(category, names_sep =\"_\") %>% \n    select(-sem_annotation)\n\n\n\n\nCode\n# key from old discipline categories to new ADCAD ontology - approved matches\nthemes <- read_csv(file.path(\"data/old_themes_ADCAD_key.csv\"))\n\n# read in csv with previous classifications\nold_cat <- read.csv(file.path(\"data/dataset_categorization.csv\"))\n\n# replace old theme categories with new ADCAD ontology terms.\n# only replacing terms that don't need review (themes df), leave the rest. \nnew_cat <- old_cat %>% \n  select(url, id)\n\nfor(i in colnames(old_cat[7:11])) {\n  # sequence through column names\n  new_cat[[i]] <- purrr::map(old_cat[[i]], function(old_theme) {\n    if (old_theme %in% themes$old) {\n      # replace if old theme is listed\n      new_theme <-\n        themes$ADCAD[which(themes$old == old_theme)] # get corresponding new disc\n      return(new_theme) # use disc ontology term to populate column\n    } else{\n      if (old_theme == \"\") {\n        old_theme = NA\n      }\n      return(old_theme)\n    } # keep old_theme if review is needed\n  })\n}\n\n# unnest list columns\nnew_cat %<>% \n  unnest(cols = c(\"theme1\", \"theme2\", \"theme3\", \"theme4\", \"theme5\"))\n\n# Add re-categorization to solr query results (adc_disc_wide)\nadc_disc_new <- adc_disc_wide %>% \n  left_join(new_cat, by = \"id\") %>% \n  mutate(url = url)\n\n# check if there is any overlapping entries between theme1 & category_1 etc.\n# overlap_cat1 <- adc_disc_new %>%\n#   filter(!is.na(theme1) & !is.na(category_1))\n# \n# overlap_cat2 <- adc_disc_new %>%\n#   filter(!is.na(theme2) & !is.na(category_2))\n# \n# overlap_cat3 <- adc_disc_new %>%\n#   filter(!is.na(theme3) & !is.na(category_3))\n# \n# overlap_cat4 <- adc_disc_new %>%\n#   filter(!is.na(theme4) & !is.na(category_4))\n# \n# overlap_cat5 <- adc_disc_new %>%\n#   filter(!is.na(theme5) & !is.na(category_5))\n# ^^ No overlap between two datasets\n\n# Combine discipline category columns + DOI\nadc_disc_ADCAD <- adc_disc_new %>% \n  transmute(url = url,\n            id = id,\n            title = title,\n            disc_cat_1 = ifelse(!is.na(category_1), category_1, theme1),\n            disc_cat_2 = ifelse(!is.na(category_2), category_2, theme2),\n            disc_cat_3 = ifelse(!is.na(category_3), category_3, theme3),\n            disc_cat_4 = ifelse(!is.na(category_4), category_4, theme4),\n            disc_cat_5 = ifelse(!is.na(category_5), category_5, theme5)\n  )\n# Save results\n# write.csv(adc_disc_ADCAD, \"./output/ADC_ADCAD_disciplines.csv\", row.names = F)"
  }
]