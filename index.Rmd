---
title: "Arctic Data Center Dataset Citation Analysis"
subtitle: "Data Fellowship Project 2022"
author: "Althea Marks"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_collapsed: true
    toc_depth: 3
    theme: spacelab
---

### Purpose

Run ADC DOIs through `scythe` & compare to known DataONE metrics citations. Known ADC citations have mixed origins including DataCite, pervious `scythe` runs, and mannual additions via the ADC UI.

### Questions

1)  Does the addition of the xDD digital library to the Scythe package improve the quality and scope of citations in the ADC?

2)  Does increasing the number of sources we are searching result in more complete coverage (quality)?

-   Overlap in citation among sources
-   Species rarification curve inspired - start to get to a point where we can estimate the actual amount of citation out there. Dataset citations are rare enough the technique may not be applicable.

3)  Does the prevalence of data citations differ among disciplines (enviro vs SS)?

-   Use ADC dicipline classifications
-   Dataset citations are rare, N of classifications varies widely, need to control for sampling biases <https://zenodo.org/record/4730857#.YoaQ2WDMKrM>

4)  Total number of citations is extremely useful. Ground truth analysis - for a small number of datasets manually search through literature for citations.

5)  Do usage metrics (downloads and views) correlate well with citation metrics?

### Methods Overview

1)  Gather existing/known ADC dataset citations picked up by the automated DataONE metrics API
2)  Get a list of all ADC dataset DOIs
3)  Run all ADC dataset DOIs through `scythe` libraries
4)  Review HTTP errors and rerun
5)  Calculate citation source overlap
5)  Compare citations from `scythe` to DataONE metrics

### Setup

```{r setup, include=F}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE, message = FALSE)

```

```{r directories, eval=TRUE}
output_directory <- "./output"

```

```{r package_library, eval=TRUE}
library(readr)
library(jsonlite)
library(tidyr)
library(dplyr)
library(dataone)
library(job)
library(magrittr)
library(readr)
# library(scythe) # published version of package

# install development version of scythe package with added xdd library
devtools::install_github("dataoneorg/scythe@develop")
```

### Analysis

#### 1. Retrieve all ADC citations with GET API request with the following request body

metrics service production endpoint: <https://logproc-stage-ucsb-1.test.dataone.org/metrics>
documentation: <https://app.swaggerhub.com/apis/nenuji/data-metrics/1.0.0.3>

```{r get_request_citations}
{
  "metricsPage":{
    "total":0,
    "start":0,
    "count":0
  },
  "metrics":["citations"],
  "filterBy":[{
    "filterType":"repository",
    "values":["urn:node:ARCTIC"],
    "interpretAs":"list"
  },
  {
    "filterType":"month",
    "values":["01/01/2012",
              "05/24/2022"],
    "interpretAs":"range"
  }],
  "groupBy":["month"]
}
```

Example request:

```{r example_request}
https://logproc-stage-ucsb-1.test.dataone.org/metrics?metricsRequest={%22metricsPage%22:{%22total%22:0,%22start%22:0,%22count%22:0},%22metrics%22:[%22citations%22],%22filterBy%22:[{%22filterType%22:%22repository%22,%22values%22:[%22urn:node:ARCTIC%22],%22interpretAs%22:%22list%22},{%22filterType%22:%22month%22,%22values%22:[%2201/01/2012%22,%2205/24/2022%22],%22interpretAs%22:%22range%22}],%22groupBy%22:[%22month%22]
}
```

Function to retrieve dataset citations generated by DataOne metrics service (from Jeanette Clark):

```{r adc_metrics_citations_function}

metrics_citations <- function(from = as.POSIXct("2010-01-01"), to = as.POSIXct(Sys.Date())){

    from <- as.Date(from); to <- as.Date(to)
    from_q <- paste(stringr::str_pad(lubridate::month(from), 2, side = "left", pad = "0"),
                    stringr::str_pad(lubridate::day(from), 2, side = "left", pad = "0"),
                    stringr::str_pad(lubridate::year(from), 2, side = "left", pad = "0"),
                    sep = "/")

    to_q <- paste(stringr::str_pad(lubridate::month(to), 2, side = "left", pad = "0"),
                  stringr::str_pad(lubridate::day(to), 2, side = "left", pad = "0"),
                  stringr::str_pad(lubridate::year(to), 2, side = "left", pad = "0"),
                  sep = "/")

    d <- jsonlite::fromJSON(paste0('https://logproc-stage-ucsb-1.test.dataone.org/metrics?q={%22metricsPage%22:{%22total%22:0,%22start%22:0,%22count%22:0},%22metrics%22:[%22citations%22],%22filterBy%22:[{%22filterType%22:%22repository%22,%22values%22:[%22urn:node:ARCTIC%22],%22interpretAs%22:%22list%22},{%22filterType%22:%22month%22,%22values%22:[%22', from_q,'%22,%22', to_q, '%22],%22interpretAs%22:%22range%22}],%22groupBy%22:[%22month%22]}'))

    output_json <- d$resultDetails$citations # pulls citation info
    output_df <- as.data.frame(do.call(rbind, output_json), row.names = FALSE) # binds nested cit info into dataframe
   # output_clean <- rownames_to_column(output_df, var = "citation_id") # converts row names to column
    return(output_df)
}
```

```{r adc_citations, cache=TRUE}
# Run ADC API Get call, unnest target_id results to individual columns
dataone_cit <- metrics_citations()

dataone_cit <- tidyr::unnest(dataone_cit,
                          cols = c(target_id, source_id, source_url,
                                   link_publication_date, origin, title,
                                   publisher, journal, volume, page, year_of_publishing))

write_csv(dataone_cit, file.path(output_directory,
                                 paste0("dataone_metrics_cit_", Sys.Date(),".csv")))
```

#### 2. Get all ADC DOIs from SOLR query

DataOne metrics API can only provide data package DOIs with citations, and can not provide a comprehensive list of all data package DOIs contained within the ADC. To search through all the repository metadata we query the DataONE search index (Apache SOLR search engine). SOLR is the same underlying mechanism that DataONE uses in the online tool and can create complex logical query conditions.

```{r SOLR_query, cache=TRUE}
# complete list of searchable values
#getQueryEngineDescription(cn, "solr")

# set coordinationg node
cn <- dataone::CNode("PROD")

# point to specific member node
mn <- dataone::getMNode(cn, "urn:node:ARCTIC")

# set up Solr query parameters
queryParamList <- list(q="id:doi*", 
                       fl="id,title,dateUploaded,datasource",
                       start ="0",
                       rows = "100000")
# use `q = "identifier:doi* AND (*:* NOT obsoletedBy:*)"` to only include current versions of data packages 

# send query to Solr, return results as dataframe
solr_adc_result <- dataone::query(mn, solrQuery=queryParamList, as="data.frame", parse=FALSE)

write.csv(solr_adc_result, file.path(output_directory, 
                                     paste0("solr_adc_", Sys.Date(), ".csv")))
```

#### 3. Run all ADC dataset DOIs through `scythe` libraries

```{r All_ADC_DOIs}
# create vector of all ADC DOIs from solr query `result`
adc_all_dois <- c(solr_adc_result$id)
```

APIs can have request rate limits. These specific rates are often found in the API documentation or the API response headers. If request rate limits are exceeded API queries will fail.

```{r get_API_rate_limits}
# Scopus request Limits
key_scopus <- scythe::scythe_get_key("scopus")
url <- paste0("https://api.elsevier.com/content/search/scopus?query=ALL:",
  "10.18739/A2M32N95V",
  paste("&APIKey=", key_scopus, sep = ""))

curlGetHeaders(url)
# [15:17] shows "X-RateLimit-Limit:", "X-RateLimit-Remaining:", and "X-RateLimit-Reset:" (Unix epoch is the number of seconds that have elapsed since January 1, 1970 at midnight UTC time minus the leap seconds)

# Springer request Limits
# 300 calls/min and 5000/day
# not found in response header, received email from springer that I was exceeding their rates above

#key_spring <- scythe::scythe_get_key("springer")
#url_spring <- paste0("http://api.springernature.com/meta/v2/json?q=doi:10.1007/BF00627098&api_key=", key_spring)
#curlGetHeaders(url_spring)
```

Run each library search in parallel in separate background jobs to keep console available to work with. By default the `job::job()` imports the global environment into the background job.

**Important Note** `scythe::scythe_set_key()` is a wrapper for the `key_ring` package. An interactive password prompt is required to access the API keys stored in `key_ring`. This *does not work* within a background job environment; your keyring needs to be temporarily unlocked with `keyring::keyring_unlock("scythe", "your password")` replace `password` with your actual keyring password. Be careful not to save, commit, or push your personal keyring password.

```{r citation_searches_background_jobs}

# Run each source/library search in a separate background job. Running a for loop will return incomplete results if API query fails, which is better than loosing all progress because of a single error in a single vector call.  

key <- "password"

# Set up empty results data.frames
citations_scopus <- data.frame()
citations_springer <- data.frame()
citations_plos <- data.frame()
citations_xdd <- data.frame()


######### Scopus
job::job({
  for (i in seq_along(adc_all_dois)) {
    # access API keys within background job environment
    keyring::keyring_unlock("scythe", key)
    # suppress errors and continue loop iteration
    result <- tryCatch(citation <- scythe::citation_search(adc_all_dois[i], "scopus"),
                       error = function(err) {
                         data.frame("article_id" = NA,
                                    "article_title" = NA,
                                    "dataset_id" = adc_all_dois[i],
                                    "source" = paste0("scopus ", as.character(err)))
                         }
                       )
    citations_scopus <- rbind(citations_scopus, result)
  }
}, title = paste0("scopus citation search ", Sys.time()))


######### PLOS
job::job({
  for (i in seq_along(adc_all_dois)) {
    # access API keys within background job environment
    keyring::keyring_unlock("scythe", key)
    # suppress errors and continue loop iteration
    result <- tryCatch(citation <- scythe::citation_search(adc_all_dois[i], "plos"),
                       error = function(err) {
                         data.frame("article_id" = NA,
                                    "article_title" = NA,
                                    "dataset_id" = adc_all_dois[i],
                                    "source" = paste0("plos", as.character(err)))
                         }
                       )
    citations_plos <- rbind(citations_plos, result)
    }
}, title = paste0("plos citation search ", Sys.time()))


########## XDD
job::job({
  for (i in seq_along(adc_all_dois)) {
    # access API keys within background job environment
    keyring::keyring_unlock("scythe", key)
    # suppress errors and continue loop iteration
    result <- tryCatch(citation <- scythe::citation_search(adc_all_dois[i], "xdd"),
                       error = function(err) {
                         data.frame("article_id" = NA,
                                    "article_title" = NA,
                                    "dataset_id" = adc_all_dois[i],
                                    "source" = paste0("xdd", as.character(err)))
                         }
                       )
    citations_xdd <- rbind(citations_xdd, result)
    }
}, title = paste0("xdd citation search ", Sys.time()))
```

Springer's API query limits affected how we ran our search. We decided to break the list of ADC DOIs into < 5,000 DOI chunks and run each chunk manually through the API with 24hrs in between the last query and starting the next DOI chunk. We could have changed the base `scythe` function `citation_search_springer()` to slow down to accommodate both request limits, but this would substantially slow down the function and make smaller DOIs queries slow and cumbersome. 

```{r springer_search_throttled}
######### Springer

# divide ADC corpus into chunks less than Springer's 5,000/day request limit
springer_limit <- 4995
length(adc_all_dois) / springer_limit
chunk_1 <- adc_all_dois[1:springer_limit]
chunk_2 <- adc_all_dois[springer_limit:(springer_limit*2)]
chunk_3 <- adc_all_dois[(springer_limit*2):length(adc_all_dois)]

# change "chunk_x" object to search next chunk of DOIs. Must wait 24 hrs from last request. 
doi_chunk = chunk_1

job::job({
  for (i in seq_along(doi_chunk)){ 
    # access API keys within background job environment
    keyring::keyring_unlock("scythe", key)
    # suppress errors and continue loop iteration
    result <- tryCatch(citation <- scythe::citation_search(doi_chunk[i], "springer"),
                       error = function(err) {
                         data.frame("article_id" = NA,
                                    "article_title" = NA,
                                    "dataset_id" = doi_chunk[i],
                                    "source" = paste0("springer ", as.character(err)))
                         }
                       )
    citations_springer <- rbind(citations_springer, result)
    }
}, title = paste0("springer citation search", Sys.time())
)
```

#### 4. Dealing with API request errors 

The `tryCatch()` functions in the above search `for` loops records errors produced from any API request. The corresponding DOIs can be aggregated and rerun through `scythe`. The code below does this. When running the DOIs with HTTP errors through Scopus a second time we discovered a small bug in the `scythe` code which was subsequently fixed. 

```{r run_failed_query_dois_again}
doi_error_scopus <- citations_scopus[is.na(citations_scopus$article_id),"dataset_id"]$dataset_id
doi_error_springer <- citations_springer[is.na(citations_springer$article_id), "dataset_id"]$dataset_id # no errors
doi_error_plos <- citations_plos[is.na(citations_plos$article_id), "dataset_id"]$dataset_id
doi_error_xdd <- citations_xdd[is.na(citations_xdd$article_id), "dataset_id"]$dataset_id # no errors

######### Scopus # this isn't working
citations_error_scopus <- data.frame()
job::job({
  for (i in seq_along(doi_error_scopus)) {
    # access API keys within background job environment
    keyring::keyring_unlock("scythe", key)
    # suppress errors and continue loop iteration
    result <- tryCatch(citation <- scythe::citation_search(doi_error_scopus[i], "scopus"),
                       error = function(err) {
                         data.frame("article_id" = NA,
                                    "article_title" = NA,
                                    "dataset_id" = doi_error_scopus[i],
                                    "source" = paste0("scopus ", as.character(err)))
                         }
                       )
    citations_error_scopus <- rbind(citations_error_scopus, result)
  }
}, title = paste0("scopus error citation search ", Sys.time()))

# save search results from errored DOI
write_csv(citations_error_scopus, "./output/scythe_2022-07-14_scopus_error.csv")
# add error citations results to main scopus results data.frame
citations_scopus <- rbind(citations_scopus, citations_error_scopus)

######### PLOS
citations_error_plos <- data.frame()
job::job({
  for (i in seq_along(doi_error_plos)) {
    # access API keys within background job environment
    keyring::keyring_unlock("scythe", key)
    # suppress errors and continue loop iteration
    result <- tryCatch(citation <- scythe::citation_search(doi_error_plos[i], "plos"),
                       error = function(err) {
                         data.frame("article_id" = NA,
                                    "article_title" = NA,
                                    "dataset_id" = doi_error_plos[i],
                                    "source" = paste0("plos", as.character(err)))
                         }
                       )
    citations_error_plos <- rbind(citations_error_plos, result)
    }
}, title = paste0("plos error citation search ", Sys.time()))
# empty dataframe return means no citations found and no HTTP errors
```

```{r save_scythe_results}
## Save results as .csv's to project output folder
date <- "2022-07-14"

write.csv(citations_scopus,
          file.path(output_directory, paste0("scythe_", date, "_scopus_all.csv")),
          row.names = F)
write.csv(citations_springer,
          file.path(output_directory, paste0("scythe_", date, "_springer_all.csv")),
          row.names = F)
write.csv(citations_plos,
          file.path(output_directory, paste0("scythe_", date, "_plos_all.csv")),
          row.names = F)
write.csv(citations_xdd,
          file.path(output_directory, paste0("scythe_", date, "_xdd_all.csv")),
          row.names = F)

```

#### 5. Calculate citation source overlap

We evaluated the redundancy in citations found among sources by matching citations between source search results. A citation is defined by the unique combination of `article_id` and `dataset_id`. Percent overlap is the percent of total citations found in a source also found in a second source.

```{r source_overlap}
scythe_cit <-
  rbind(citations_scopus,
        citations_springer,
        citations_plos,
        citations_xdd) %>%
  filter(!is.na(article_id)) # remove NA/error observations

write_csv(scythe_cit, file.path(output_directory, paste0("scythe_", date, "_citations_all.csv")))

scythe_sum <- scythe_cit %>%
  group_by(source) %>%
  summarise("total_citations_found" = n())

# match article_id and dataset_id columns among results
overlap_plos_scopus <-
  inner_join(citations_plos,
             citations_scopus,
             by = c("article_id", "dataset_id"))
overlap_plos_springer <-
  inner_join(citations_plos,
             citations_springer,
             by = c("article_id", "dataset_id"))
overlap_plos_xdd <-
  inner_join(citations_plos,
             citations_xdd,
             by = c("article_id", "dataset_id"))
overlap_scopus_springer <-
  inner_join(citations_scopus,
             citations_springer,
             by = c("article_id", "dataset_id"))
overlap_scopus_xdd <-
  inner_join(citations_scopus,
             citations_xdd,
             by = c("article_id", "dataset_id"))
overlap_springer_xdd <-
  inner_join(citations_springer,
             citations_xdd,
             by = c("article_id", "dataset_id"))

# function to calculate percent of source citations also found in a second source
calc_prct_overlap <- function(source_1, source_2) {
  pairs <-
    inner_join(source_1, source_2, by = c("article_id", "dataset_id"))
  overlap <-
    nrow(pairs) / nrow(source_1) # percent of source_1 total also found in source_2 (pairs)
  return(overlap)
}

scythe_overlap_sum <- scythe_cit %>%
  mutate("citation_df" = ifelse(
    source == "plos",
    "citations_plos",
    ifelse(
      source == "scopus",
      "citations_scopus",
      ifelse(source == "springer", "citations_springer", "citations_xdd")
    )
  )) %>%
  group_by(source, citation_df) %>%
  summarise(
    "total_citations" = n(),
    "prct_in_plos" = calc_prct_overlap(eval(parse(text = citation_df)), 
                                              citations_plos),
    "prct_in_scopus" = calc_prct_overlap(eval(parse(text = citation_df)), 
                                                citations_scopus),
    "prct_in_springer" = calc_prct_overlap(eval(parse(text = citation_df)), 
                                                  citations_springer),
    "prct_in_xdd" = calc_prct_overlap(eval(parse(text = citation_df)), 
                                             citations_xdd)
  ) %>%
  select(1, 3:7)
# Save summary table to output folder
write.csv(scythe_overlap_sum,
          file.path(
            output_directory,
            paste0("scythe_", date, "_prct_overlap.csv")
          ),
          row.names = F)
```

```{r overlap_results_table, eval=TRUE}
source_prct_overlap_2022_07_14 <- readr::read_csv("./output/scythe_2022-07-14_prct_overlap.csv")
knitr::kable(source_prct_overlap_2022_07_14, caption = "Percent overlap between Scythe sources")
```

#### 6. How many completely unique citations found in each source?

```{r unique_citations_by_source, eval = TRUE}
citations_plos <- readr::read_csv("./output/scythe_2022-07-14_plos.csv")
citations_scopus <- readr::read_csv("./output/scythe_2022-07-14_scopus.csv")
citations_springer <- readr::read_csv("./output/scythe_2022-07-14_springer.csv")
citations_xdd <- readr::read_csv("./output/scythe_2022-07-14_xdd.csv")

unique_plos <- citations_plos %>% 
  anti_join(citations_scopus, by = c("article_id", "dataset_id")) %>% 
  anti_join(citations_springer, by = c("article_id", "dataset_id")) %>% 
  anti_join(citations_xdd, by = c("article_id", "dataset_id")) %>% 
  filter(!is.na(article_id))

unique_scopus <- citations_scopus %>% 
  anti_join(citations_plos, by = c("article_id", "dataset_id")) %>% 
  anti_join(citations_springer, by = c("article_id", "dataset_id")) %>% 
  anti_join(citations_xdd, by = c("article_id", "dataset_id")) %>% 
  filter(!is.na(article_id))

unique_springer <- citations_springer %>% 
  anti_join(citations_plos, by = c("article_id", "dataset_id")) %>% 
  anti_join(citations_scopus, by = c("article_id", "dataset_id")) %>% 
  anti_join(citations_xdd, by = c("article_id", "dataset_id")) %>% 
  filter(!is.na(article_id))
  
unique_xdd <- citations_xdd %>% 
  anti_join(citations_plos, by = c("article_id", "dataset_id")) %>% 
  anti_join(citations_scopus, by = c("article_id", "dataset_id")) %>% 
  anti_join(citations_springer, by = c("article_id", "dataset_id")) %>% 
  filter(!is.na(article_id))

unique_citations <- data.frame("source" = c("plos", "scopus", "springer", "xdd"),
                               "unique_citations" = c(nrow(unique_plos), nrow(unique_scopus), 
                                                      nrow(unique_springer), nrow(unique_xdd)))

knitr::kable(unique_citations, caption = "Number of citations only found within each source")
```

#### 7. Compare `scythe` results to DataOne Metrics citations

```{r new_scythe_citations, eval=TRUE}
dataone_met_cit <- readr::read_csv("./output/dataone_metrics_cit_2022-07-08.csv")
scythe_cit <- read_csv("./output/scythe_2022-07-14_citations_all.csv")
# source_id = 'Unique identifier to the source dataset / document / article that cited the target dataset '
# target_id = 'Unique identifier to the target DATAONE dataset. This is the dataset that was cited.'
dataone_met_cit <- dataone_met_cit %>% 
  rename("article_id" = source_id, # rename dataone metrics citations columns to match scythe results
         "dataset_id" = target_id) %>% 
  select(article_id, dataset_id)

# only show citations found by scythe that do not already appear in the DataOne Metrics service
scythe_citations_new <- anti_join(scythe_cit, dataone_met_cit, by = c("article_id", "dataset_id")) %>% 
  na.omit()

# scythe::write_citation_pairs(scythe_citations_new, 
#                              path = paste0("./output/scythe_", date,"_citation_pairs.json"))
```

### Results

1)  Does the addition of the xDD digital library to the Scythe package improve the quality and scope of citations in the ADC?

-   How many unique citations were found in xDD and not in any of the other sources? xDD found `r nrow(unique_xdd)` unique citations not found in Scoupus, Springer, or PLOS digital libraries. ## new to DataOne Metric service (should be CrossRef & ORCID reports in `dataone_citations_reporter.csv`)

2)  Does increasing the number of sources we are searching result in more complete coverage (quality)?

-   Overlap in citation among sources
-   Species rarification curve inspired - start to get to a point where we can estimate the actual amount of citation out there. Dataset citations are rare enough the technique may not be applicable.

3)  Does the prevalence of data citations differ among disciplines (enviro vs SS)?

-   Use ADC dicipline classifications
-   Dataset citations are rare, N of classifications varies widely, need to control for sampling biases <https://zenodo.org/record/4730857#.YoaQ2WDMKrM>

4)  Total number of citations is extremely useful. Ground truth analysis - for a small number of datasets manually search through literature for citations.

5)  Do usage metrics (downloads and views) correlate well with citation metrics?

### Next Steps

-   Analysis by discipline

[Root of ADC discipline semantics annotations](https://bioportal.bioontology.org/ontologies/ADCAD/?p=classes&conceptid=root) Classes/ID is where to look for query specifics. "Here's an example SOLR query that looks for two of those disciplines: `https://cn.dataone.org/cn/v2/query/solr/?q=sem_annotation:*ADCAD_00077+OR+sem_annotation:*ADCAD_00005&fl=identifier,formatId,sem_annotation`" - Matt Need to list every single SS ID in query list, not set up to query umbrella SS ID just yet

```{r discipline_semantics}
# Run second Solr query to pull semantic annotations for 2022_08_10 DOIs

# set coordinationg node
cn <- dataone::CNode("PROD")

# point to specific member node
mn <- dataone::getMNode(cn, "urn:node:ARCTIC")

# set up Solr query parameters
queryParamList <- list(q="id:doi*", 
                       fl="id,title,dateUploaded,datasource,sem_annotation",
                       start ="0",
                       rows = "100000")

# send query to Solr, return results as dataframe. parse = T returns list column, F returns chr value
solr_adc_sem <- dataone::query(mn, solrQuery=queryParamList, as="data.frame", parse=T)

# read in csv with coded discipline ontology
adc_disc <- read.csv("https://raw.githubusercontent.com/NCEAS/adc-disciplines/main/adc-disciplines.csv") %>% 
    mutate(an_uri = paste0("https://purl.dataone.org/odo/ADCAD_", stringr::str_pad(id, 5, "left", pad = "0")))

solr_adc_sem$category <- purrr::map(solr_adc_sem$sem_annotation, function(x){
    t <- eval(parse(text = grep("*ADCAD*", x, value = TRUE)))
    cats <- c()
    for (i in 1:length(t)){
        z <- which(adc_disc$an_uri == t[i])
        cats[i] <- adc_disc$discipline[z]
        
    }
    return(cats)
})

## test
testfun <- function(x){
    t <- grep("*ADCAD*", x, value = TRUE)
    cats <- c()
    for (i in 1:length(t)){
        z <- which(adc_disc$an_uri == t[i])
        cats[i] <- adc_disc$discipline[z]
        
    }
    return(cats)
}




res$category <- purrr::map(res$sem_annotation, function(x){
    t <- grep("*ADCAD*", x, value = TRUE)
    cats <- c()
    for (i in 1:length(t)){
        z <- which(adc_disc$an_uri == t[i])
        cats[i] <- adc_disc$discipline[z]
        
    }
    return(cats)
})

res_wide <- res %>% 
    unnest_wider(category, names_sep = "category") %>% 
    select(-sem_annotation)
```

